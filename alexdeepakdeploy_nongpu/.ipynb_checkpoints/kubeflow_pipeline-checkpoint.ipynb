{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df501dd7-7152-4677-911d-cf6464d59198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ce154c-c86e-464e-857d-f0a5680fd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6460508-381f-4cfd-a105-3f8198113abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041cfab8-e324-4fe4-9631-b6be5ae3a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer_image_vertex2/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer_image_vertex2/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf-gpu.2-8\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY model.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"model.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "add36ad2-cd84-46fc-865c-f7ab58e7d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./trainer_image_vertex2/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./trainer_image_vertex2/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "import numpy as np\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import credentials\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (Conv1D, Dense, Dropout, Flatten, MaxPooling1D, Softmax)\n",
    "\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "MODEL_FILENAME = \"model.pkl\"\n",
    "\n",
    "def get_blob(blobs):\n",
    "    for blob in blobs:\n",
    "        yield blob\n",
    "        \n",
    "def get_image_paths(image_input_dir):\n",
    "    # initialize the GCS client\n",
    "    image_bucket = image_input_dir.split('/')[2]\n",
    "    prefix_dir = '/'.join(image_input_dir.split('/')[3:])\n",
    "    storage_client = storage.Client()\n",
    "    # get the storage bucket\n",
    "    bucket = storage_client.get_bucket(image_bucket)\n",
    "   \n",
    "\n",
    "    image_paths=[]\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(image_bucket, prefix=prefix_dir)\n",
    "    \n",
    "    for blob in get_blob(blobs):\n",
    "        if \"output\" in blob.name:\n",
    "            image_paths.append('gs://spectrain_new/'+blob.name)\n",
    "    return image_paths\n",
    "\n",
    "def load_images(imagePath):\n",
    "    # read the image from disk, decode it, convert the data type to\n",
    "    # floating point, and resize it\n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, (256,256))\n",
    "    # parse the class label from the file path\n",
    "    label = tf.strings.split(imagePath, os.path.sep)[-2]\n",
    "    if label=='positive':\n",
    "        label=1\n",
    "    else:\n",
    "        label=0\n",
    "    # return the image and the label\n",
    "    return (image, label)\n",
    "\n",
    "    # return the image and the label\n",
    "    return (image, label)\n",
    "\n",
    "def load_dataset(images_dir, batch_size, training):\n",
    "    filePaths = get_image_paths(image_input_dir=images_dir)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(filePaths)\n",
    "    ds = (ds\n",
    "        .map(load_images)\n",
    "        .cache()\n",
    "        .shuffle(len(filePaths))\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        return ds.repeat()\n",
    "    else:\n",
    "        return ds\n",
    "\n",
    "def build_model(filter_size_1, filter_size_2, kernel_size, pool_kernel_size, hidden_units_1, hidden_units_2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu', input_shape=(256, 256), padding='same'))\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size, padding='same'))\n",
    "    model.add(Conv1D(filter_size_2, kernel_size=kernel_size,activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['AUC'],\n",
    "                  run_eagerly=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "# Instantiate the HyperTune reporting object\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "# Reporting callback\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='auc',\n",
    "            metric_value=logs['val_auc'],\n",
    "            global_step=epoch)\n",
    "        \n",
    "        \n",
    "def train_and_evaluate(train_data_path,\n",
    "                    eval_data_path,                \n",
    "                    filt_size1,\n",
    "                    filt_size2, \n",
    "                    nnsize_1,\n",
    "                    nnsize_2,batch_size, hptune):\n",
    "    num_epochs=20\n",
    "    train_examples=5000\n",
    "    eval_steps=100\n",
    "    filt_size1 = int(filt_size1)\n",
    "    filt_size2 = int(filt_size2)\n",
    "    ksize = 4\n",
    "    pool_ksize = 2\n",
    "    nnsize_1 = int(nnsize_1)\n",
    "    nnsize_2 = int(nnsize_2)\n",
    "    batch_size = int(batch_size)\n",
    "    model = build_model(filter_size_1=filt_size1, filter_size_2=filt_size2, \n",
    "                        kernel_size=ksize, pool_kernel_size=pool_ksize\n",
    "                        , hidden_units_1=nnsize_1, hidden_units_2=nnsize_2)\n",
    "\n",
    "    trainds = load_dataset(train_data_path, batch_size, training=True)\n",
    "\n",
    "    evalds = load_dataset(eval_data_path, batch_size, training=False)\n",
    "    \n",
    "    \n",
    "    if eval_steps:\n",
    "        evalds = evalds.take(count=eval_steps)\n",
    "\n",
    "    num_batches = batch_size * num_epochs\n",
    "    steps_per_epoch = train_examples // batch_size\n",
    "   \n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,\n",
    "        callbacks=[HPTCallback()])\n",
    "    \n",
    "    if not hptune:\n",
    "        tf.saved_model.save(obj=model, export_dir=AIP_MODEL_DIR) \n",
    "        \n",
    "        print(\"Exported trained model to {}\".format(AIP_MODEL_DIR))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_and_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d553dea-ac82-4717-908b-46a6302a56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8021f61-db29-4aa8-8c14-9ae7461be4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex:latest'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_spectrain_vertex\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51ccd6de-65ff-4a89-aa77-778025e323c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 10.3 KiB before compression.\n",
      "Uploading tarball of [trainer_image_vertex2] to [gs://qwiklabs-asl-00-c812c3b423f2_cloudbuild/source/1686921239.40684-9b0ca264851b45bba3e1f461997af6ea.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-00-c812c3b423f2/locations/global/builds/65634dc6-adfb-4c64-9e28-083a316ce004].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/65634dc6-adfb-4c64-9e28-083a316ce004?project=469700469475 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"65634dc6-adfb-4c64-9e28-083a316ce004\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-00-c812c3b423f2_cloudbuild/source/1686921239.40684-9b0ca264851b45bba3e1f461997af6ea.tgz#1686921239670924\n",
      "Copying gs://qwiklabs-asl-00-c812c3b423f2_cloudbuild/source/1686921239.40684-9b0ca264851b45bba3e1f461997af6ea.tgz#1686921239670924...\n",
      "/ [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
      "Operation completed over 1 objects/2.3 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  14.85kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf-gpu.2-8\n",
      "latest: Pulling from deeplearning-platform-release/tf-gpu.2-8\n",
      "846c0b181fff: Pulling fs layer\n",
      "6fc9dd88827c: Pulling fs layer\n",
      "0b311d7060d0: Pulling fs layer\n",
      "326d76058f67: Pulling fs layer\n",
      "ed7e4c52c661: Pulling fs layer\n",
      "4f05f5570c7a: Pulling fs layer\n",
      "ab264e292103: Pulling fs layer\n",
      "527d1d5ab821: Pulling fs layer\n",
      "fc46d4e99009: Pulling fs layer\n",
      "9ccf692754fa: Pulling fs layer\n",
      "c6611ece70c4: Pulling fs layer\n",
      "fe6ff819b45f: Pulling fs layer\n",
      "1b492c2d74c8: Pulling fs layer\n",
      "821f163f6bbf: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "36dd0a54661c: Pulling fs layer\n",
      "17c622f8ec96: Pulling fs layer\n",
      "78d83c0bc7f0: Pulling fs layer\n",
      "9003edfeafdf: Pulling fs layer\n",
      "e759d17f293e: Pulling fs layer\n",
      "61b61473399f: Pulling fs layer\n",
      "812089392553: Pulling fs layer\n",
      "131ab2239805: Pulling fs layer\n",
      "52af898574c9: Pulling fs layer\n",
      "a64c015fc8f2: Pulling fs layer\n",
      "f029b63dc5a5: Pulling fs layer\n",
      "90e562b6b632: Pulling fs layer\n",
      "de1cae14b2f1: Pulling fs layer\n",
      "b906272ecd14: Pulling fs layer\n",
      "554b16971540: Pulling fs layer\n",
      "38b54b6f438a: Pulling fs layer\n",
      "ab1dd146d8d3: Pulling fs layer\n",
      "e0a89f09a46f: Pulling fs layer\n",
      "004a2bcdb245: Pulling fs layer\n",
      "05a650823a22: Pulling fs layer\n",
      "7c1170e34932: Pulling fs layer\n",
      "174fac25dbc8: Pulling fs layer\n",
      "ed7e4c52c661: Waiting\n",
      "4f05f5570c7a: Waiting\n",
      "ab264e292103: Waiting\n",
      "527d1d5ab821: Waiting\n",
      "fc46d4e99009: Waiting\n",
      "9ccf692754fa: Waiting\n",
      "c6611ece70c4: Waiting\n",
      "fe6ff819b45f: Waiting\n",
      "1b492c2d74c8: Waiting\n",
      "821f163f6bbf: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "36dd0a54661c: Waiting\n",
      "17c622f8ec96: Waiting\n",
      "78d83c0bc7f0: Waiting\n",
      "9003edfeafdf: Waiting\n",
      "e759d17f293e: Waiting\n",
      "61b61473399f: Waiting\n",
      "812089392553: Waiting\n",
      "131ab2239805: Waiting\n",
      "52af898574c9: Waiting\n",
      "a64c015fc8f2: Waiting\n",
      "f029b63dc5a5: Waiting\n",
      "90e562b6b632: Waiting\n",
      "de1cae14b2f1: Waiting\n",
      "b906272ecd14: Waiting\n",
      "554b16971540: Waiting\n",
      "38b54b6f438a: Waiting\n",
      "ab1dd146d8d3: Waiting\n",
      "e0a89f09a46f: Waiting\n",
      "004a2bcdb245: Waiting\n",
      "05a650823a22: Waiting\n",
      "7c1170e34932: Waiting\n",
      "174fac25dbc8: Waiting\n",
      "326d76058f67: Waiting\n",
      "6fc9dd88827c: Verifying Checksum\n",
      "6fc9dd88827c: Download complete\n",
      "0b311d7060d0: Download complete\n",
      "326d76058f67: Verifying Checksum\n",
      "326d76058f67: Download complete\n",
      "ed7e4c52c661: Verifying Checksum\n",
      "ed7e4c52c661: Download complete\n",
      "846c0b181fff: Verifying Checksum\n",
      "846c0b181fff: Download complete\n",
      "ab264e292103: Verifying Checksum\n",
      "ab264e292103: Download complete\n",
      "fc46d4e99009: Download complete\n",
      "527d1d5ab821: Download complete\n",
      "c6611ece70c4: Verifying Checksum\n",
      "c6611ece70c4: Download complete\n",
      "846c0b181fff: Pull complete\n",
      "6fc9dd88827c: Pull complete\n",
      "4f05f5570c7a: Download complete\n",
      "0b311d7060d0: Pull complete\n",
      "326d76058f67: Pull complete\n",
      "ed7e4c52c661: Pull complete\n",
      "1b492c2d74c8: Verifying Checksum\n",
      "1b492c2d74c8: Download complete\n",
      "821f163f6bbf: Verifying Checksum\n",
      "821f163f6bbf: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "9ccf692754fa: Verifying Checksum\n",
      "9ccf692754fa: Download complete\n",
      "36dd0a54661c: Verifying Checksum\n",
      "36dd0a54661c: Download complete\n",
      "78d83c0bc7f0: Verifying Checksum\n",
      "78d83c0bc7f0: Download complete\n",
      "9003edfeafdf: Verifying Checksum\n",
      "9003edfeafdf: Download complete\n",
      "e759d17f293e: Verifying Checksum\n",
      "e759d17f293e: Download complete\n",
      "61b61473399f: Verifying Checksum\n",
      "61b61473399f: Download complete\n",
      "812089392553: Verifying Checksum\n",
      "812089392553: Download complete\n",
      "17c622f8ec96: Verifying Checksum\n",
      "17c622f8ec96: Download complete\n",
      "52af898574c9: Verifying Checksum\n",
      "52af898574c9: Download complete\n",
      "a64c015fc8f2: Verifying Checksum\n",
      "a64c015fc8f2: Download complete\n",
      "f029b63dc5a5: Verifying Checksum\n",
      "f029b63dc5a5: Download complete\n",
      "90e562b6b632: Verifying Checksum\n",
      "90e562b6b632: Download complete\n",
      "de1cae14b2f1: Verifying Checksum\n",
      "de1cae14b2f1: Download complete\n",
      "b906272ecd14: Verifying Checksum\n",
      "b906272ecd14: Download complete\n",
      "554b16971540: Download complete\n",
      "38b54b6f438a: Verifying Checksum\n",
      "38b54b6f438a: Download complete\n",
      "ab1dd146d8d3: Verifying Checksum\n",
      "ab1dd146d8d3: Download complete\n",
      "e0a89f09a46f: Verifying Checksum\n",
      "e0a89f09a46f: Download complete\n",
      "131ab2239805: Verifying Checksum\n",
      "131ab2239805: Download complete\n",
      "fe6ff819b45f: Verifying Checksum\n",
      "fe6ff819b45f: Download complete\n",
      "7c1170e34932: Verifying Checksum\n",
      "7c1170e34932: Download complete\n",
      "174fac25dbc8: Verifying Checksum\n",
      "174fac25dbc8: Download complete\n",
      "004a2bcdb245: Verifying Checksum\n",
      "004a2bcdb245: Download complete\n",
      "05a650823a22: Verifying Checksum\n",
      "05a650823a22: Download complete\n",
      "4f05f5570c7a: Pull complete\n",
      "ab264e292103: Pull complete\n",
      "527d1d5ab821: Pull complete\n",
      "fc46d4e99009: Pull complete\n",
      "9ccf692754fa: Pull complete\n",
      "c6611ece70c4: Pull complete\n",
      "fe6ff819b45f: Pull complete\n",
      "1b492c2d74c8: Pull complete\n",
      "821f163f6bbf: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "17c622f8ec96: Pull complete\n",
      "78d83c0bc7f0: Pull complete\n",
      "9003edfeafdf: Pull complete\n",
      "e759d17f293e: Pull complete\n",
      "61b61473399f: Pull complete\n",
      "812089392553: Pull complete\n",
      "131ab2239805: Pull complete\n",
      "52af898574c9: Pull complete\n",
      "a64c015fc8f2: Pull complete\n",
      "f029b63dc5a5: Pull complete\n",
      "90e562b6b632: Pull complete\n",
      "de1cae14b2f1: Pull complete\n",
      "b906272ecd14: Pull complete\n",
      "554b16971540: Pull complete\n",
      "38b54b6f438a: Pull complete\n",
      "ab1dd146d8d3: Pull complete\n",
      "e0a89f09a46f: Pull complete\n",
      "004a2bcdb245: Pull complete\n",
      "05a650823a22: Pull complete\n",
      "7c1170e34932: Pull complete\n",
      "174fac25dbc8: Pull complete\n",
      "Digest: sha256:e4dffa3b30f202359fdd1f2e3cd444115ff155956aa4b575f7b60fe1a5a5a140\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf-gpu.2-8:latest\n",
      " ---> 16b0bfb454cc\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in b9ac6cca2e6c\n",
      "Collecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 4.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire) (2.3.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=cb3b258a17073406d3ce548e3714d90225e46663b5a4eae8d6a27a44c39f0cad\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3973 sha256=4f3d7cadde4690bcbdebe41b6d96662976e8b33bcdafd62190dfe4cc1247782c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, fire\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.5.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container b9ac6cca2e6c\n",
      " ---> d3eba4ef224b\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 4616df5dec6c\n",
      "Removing intermediate container 4616df5dec6c\n",
      " ---> 1fb3427a42e0\n",
      "Step 4/5 : COPY model.py .\n",
      " ---> ec097ac426fe\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"model.py\"]\n",
      " ---> Running in 0d430970cee0\n",
      "Removing intermediate container 0d430970cee0\n",
      " ---> 665ea2ece2d9\n",
      "Successfully built 665ea2ece2d9\n",
      "Successfully tagged gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex]\n",
      "be252be11b09: Preparing\n",
      "6bac9652ae7d: Preparing\n",
      "7584fa5c453b: Preparing\n",
      "680805195872: Preparing\n",
      "1dc70eaa3817: Preparing\n",
      "761cd481813a: Preparing\n",
      "1955fe06c167: Preparing\n",
      "db5de1786e99: Preparing\n",
      "375e10077256: Preparing\n",
      "c422bacaed9d: Preparing\n",
      "b6b99c0d93e6: Preparing\n",
      "687548e6377b: Preparing\n",
      "42edfe0748b0: Preparing\n",
      "f99df9dc1d82: Preparing\n",
      "4d598f9e30d7: Preparing\n",
      "176360cc07e0: Preparing\n",
      "ded8f7486534: Preparing\n",
      "03cc447adccd: Preparing\n",
      "5df3c6bdcfe8: Preparing\n",
      "1fc4eedb9be6: Preparing\n",
      "212b2abf9267: Preparing\n",
      "0739694b598e: Preparing\n",
      "7893569bc1e0: Preparing\n",
      "7e6443965068: Preparing\n",
      "dcee5f912448: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1743139ae39b: Preparing\n",
      "f3ffa17f7942: Preparing\n",
      "b3936e4c67d2: Preparing\n",
      "07d37209b7a9: Preparing\n",
      "fed0e8aa65b5: Preparing\n",
      "ad8fec0b36f1: Preparing\n",
      "3a217af3edf9: Preparing\n",
      "3297f5de02be: Preparing\n",
      "f3717d7fdfb7: Preparing\n",
      "e1eace4c0976: Preparing\n",
      "959a7375cb04: Preparing\n",
      "d79c672e1e8b: Preparing\n",
      "7b7c9e761223: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "212b2abf9267: Waiting\n",
      "0739694b598e: Waiting\n",
      "7893569bc1e0: Waiting\n",
      "7e6443965068: Waiting\n",
      "dcee5f912448: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "1743139ae39b: Waiting\n",
      "f3ffa17f7942: Waiting\n",
      "b3936e4c67d2: Waiting\n",
      "07d37209b7a9: Waiting\n",
      "fed0e8aa65b5: Waiting\n",
      "ad8fec0b36f1: Waiting\n",
      "3a217af3edf9: Waiting\n",
      "3297f5de02be: Waiting\n",
      "f3717d7fdfb7: Waiting\n",
      "e1eace4c0976: Waiting\n",
      "959a7375cb04: Waiting\n",
      "d79c672e1e8b: Waiting\n",
      "7b7c9e761223: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "b6b99c0d93e6: Waiting\n",
      "687548e6377b: Waiting\n",
      "42edfe0748b0: Waiting\n",
      "f99df9dc1d82: Waiting\n",
      "4d598f9e30d7: Waiting\n",
      "176360cc07e0: Waiting\n",
      "ded8f7486534: Waiting\n",
      "03cc447adccd: Waiting\n",
      "761cd481813a: Waiting\n",
      "1955fe06c167: Waiting\n",
      "db5de1786e99: Waiting\n",
      "375e10077256: Waiting\n",
      "c422bacaed9d: Waiting\n",
      "5df3c6bdcfe8: Waiting\n",
      "1fc4eedb9be6: Waiting\n",
      "1dc70eaa3817: Layer already exists\n",
      "680805195872: Layer already exists\n",
      "761cd481813a: Layer already exists\n",
      "1955fe06c167: Layer already exists\n",
      "375e10077256: Layer already exists\n",
      "db5de1786e99: Layer already exists\n",
      "b6b99c0d93e6: Layer already exists\n",
      "c422bacaed9d: Layer already exists\n",
      "687548e6377b: Layer already exists\n",
      "42edfe0748b0: Layer already exists\n",
      "f99df9dc1d82: Layer already exists\n",
      "4d598f9e30d7: Layer already exists\n",
      "176360cc07e0: Layer already exists\n",
      "ded8f7486534: Layer already exists\n",
      "03cc447adccd: Layer already exists\n",
      "5df3c6bdcfe8: Layer already exists\n",
      "1fc4eedb9be6: Layer already exists\n",
      "0739694b598e: Layer already exists\n",
      "212b2abf9267: Layer already exists\n",
      "7893569bc1e0: Layer already exists\n",
      "7e6443965068: Layer already exists\n",
      "dcee5f912448: Layer already exists\n",
      "1743139ae39b: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "b3936e4c67d2: Layer already exists\n",
      "f3ffa17f7942: Layer already exists\n",
      "07d37209b7a9: Layer already exists\n",
      "fed0e8aa65b5: Layer already exists\n",
      "ad8fec0b36f1: Layer already exists\n",
      "3a217af3edf9: Layer already exists\n",
      "3297f5de02be: Layer already exists\n",
      "f3717d7fdfb7: Layer already exists\n",
      "be252be11b09: Pushed\n",
      "e1eace4c0976: Layer already exists\n",
      "959a7375cb04: Layer already exists\n",
      "d79c672e1e8b: Layer already exists\n",
      "7b7c9e761223: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "6bac9652ae7d: Pushed\n",
      "7584fa5c453b: Pushed\n",
      "latest: digest: sha256:1c725c6a66f2756f480a9be37e6d688c24653fcc63b1cde78c65c74f71fcf746 size: 8708\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                     IMAGES                                                                        STATUS\n",
      "65634dc6-adfb-4c64-9e28-083a316ce004  2023-06-16T13:13:59+00:00  10M57S    gs://qwiklabs-asl-00-c812c3b423f2_cloudbuild/source/1686921239.40684-9b0ca264851b45bba3e1f461997af6ea.tgz  gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI trainer_image_vertex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faab79a0-919b-4129-9092-24437c2fec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415b2d60-1eb4-47a1-aedb-3744679e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./pipeline_vertex/pipeline_prebuilt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ed7c24-2891-4de2-899d-db763c3346f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BUCKET=spectrain_new\n"
     ]
    }
   ],
   "source": [
    "BUCKET = !gcloud storage ls\n",
    "BUCKET = BUCKET[-1].split(\"//\")[-1]\n",
    "BUCKET = BUCKET[:-1]\n",
    "\n",
    "%env BUCKET={BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61dc699b-0b46-4d37-b2eb-9e6f74f70798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline_vertex/pipeline_prebuilt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_vertex/pipeline_prebuilt.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "import os\n",
    "\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "from google_cloud_pipeline_components.experimental import (\n",
    "    hyperparameter_tuning_job,\n",
    ")\n",
    "from google_cloud_pipeline_components.experimental.custom_job import (\n",
    "    CustomTrainingJobOp,\n",
    ")\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_MACHINE_TYPE = os.getenv(\"SERVING_MACHINE_TYPE\", \"n1-standard-4\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"20\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"covertype\")\n",
    "BASE_OUTPUT_DIR = os.getenv(\"BASE_OUTPUT_DIR\", PIPELINE_ROOT)\n",
    "MODEL_DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-kfp-pipeline\",\n",
    "    description=\"Kubeflow pipeline that tunes, trains, and deploys on Vertex\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def create_pipeline():\n",
    "    worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-4\",\n",
    "                \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "                \"accelerator_count\": 1,\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": TRAINING_CONTAINER_IMAGE_URI,\n",
    "                \"args\": [\n",
    "                    f\"--train_data_path={TRAINING_FILE_PATH}\",\n",
    "                    f\"--eval_data_path={VALIDATION_FILE_PATH}\",\n",
    "                    # hptune\n",
    "                    \"--hptune\",\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    metric_spec = hyperparameter_tuning_job.serialize_metrics(\n",
    "       {\"auc\": \"maximize\"}\n",
    "   )\n",
    "\n",
    "    parameter_spec = hyperparameter_tuning_job.serialize_parameters(\n",
    "       {\n",
    "           \n",
    "           \"filt_size1\": hpt.DiscreteParameterSpec(\n",
    "               values=[16, 32, 64], scale=None\n",
    "           ),\n",
    "           \"filt_size2\": hpt.DiscreteParameterSpec(\n",
    "               values=[8, 16, 32], scale=None\n",
    "           ),\n",
    "           \"nnsize_1\": hpt.DiscreteParameterSpec(\n",
    "               values=[128, 256, 512], scale=None\n",
    "           ),\n",
    "           \"nnsize_2\": hpt.DiscreteParameterSpec(\n",
    "               values=[64, 128, 256], scale=None\n",
    "           ),\n",
    "           \"batch_size\": hpt.DiscreteParameterSpec(\n",
    "               values=[32, 64], scale=None\n",
    "           ),\n",
    "       }\n",
    "   )\n",
    "\n",
    "    hp_tuning_task = hyperparameter_tuning_job.HyperparameterTuningJobRunOp(\n",
    "       display_name=f\"{PIPELINE_NAME}-kfp-tuning-job\",\n",
    "       project=PROJECT_ID,\n",
    "       location=REGION,\n",
    "       worker_pool_specs=worker_pool_specs,\n",
    "       study_spec_metrics=metric_spec,\n",
    "       study_spec_parameters=parameter_spec,\n",
    "       max_trial_count=MAX_TRIAL_COUNT,\n",
    "       parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
    "       base_output_directory=PIPELINE_ROOT,\n",
    "   )\n",
    "\n",
    "    trials_task = hyperparameter_tuning_job.GetTrialsOp(\n",
    "        gcp_resources=hp_tuning_task.outputs[\"gcp_resources\"]\n",
    "    )\n",
    "\n",
    "    best_hyperparameters_task = (\n",
    "        hyperparameter_tuning_job.GetBestHyperparametersOp(\n",
    "            trials=trials_task.output, study_spec_metrics=metric_spec\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Construct new worker_pool_specs and\n",
    "    # train new model based on best hyperparameters\n",
    "    worker_pool_specs_task = hyperparameter_tuning_job.GetWorkerPoolSpecsOp(\n",
    "        best_hyperparameters=best_hyperparameters_task.output,\n",
    "        worker_pool_specs=[\n",
    "            {\n",
    "                \"machine_spec\": {\"machine_type\": \"n1-standard-4\", \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "                \"accelerator_count\": 1},\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": TRAINING_CONTAINER_IMAGE_URI,\n",
    "                    \"args\": [\n",
    "                        f\"--train_data_path={TRAINING_FILE_PATH}\",\n",
    "                        f\"--eval_data_path={VALIDATION_FILE_PATH}\",\n",
    "                        \"--nohptune\",\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    training_task = CustomTrainingJobOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-training-job\",\n",
    "        worker_pool_specs=worker_pool_specs_task.output,\n",
    "        base_output_directory=BASE_OUTPUT_DIR,\n",
    "    )\n",
    "\n",
    "    \n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-model-upload-job\",\n",
    "        artifact_uri=f\"{BASE_OUTPUT_DIR}/model\",\n",
    "        serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    )\n",
    "    model_upload_task.after(training_task)\n",
    "\n",
    "    endpoint_create_task = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        display_name=f\"{PIPELINE_NAME}-kfp-create-endpoint-job\",\n",
    "    )\n",
    "    endpoint_create_task.after(model_upload_task)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(  # pylint: disable=unused-variable\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
    "        deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "        dedicated_resources_machine_type=SERVING_MACHINE_TYPE,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6391c4-a2d3-455a-8807-5a098556222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex:latest'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f70a507d-116f-4318-acbc-c5cc3154f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://spectrain_new/pipeline\n",
      "env: PROJECT_ID=qwiklabs-asl-00-c812c3b423f2\n",
      "env: REGION=us-central1\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-asl-00-c812c3b423f2/trainer_image_spectrain_vertex:latest\n",
      "env: TRAINING_FILE_PATH=gs://spectrain_new/bhavani/train_images\n",
      "env: VALIDATION_FILE_PATH=gs://spectrain_new/bhavani/valid_images\n",
      "env: BASE_OUTPUT_DIR=gs://spectrain_new/models/20230616161314\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{BUCKET}\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "\n",
    "TRAINING_FILE_PATH = f\"gs://{BUCKET}/bhavani/train_images\"\n",
    "VALIDATION_FILE_PATH = f\"gs://{BUCKET}/bhavani/valid_images\"\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#TIMESTAMP = 20230616134122\n",
    "BASE_OUTPUT_DIR = f\"{ARTIFACT_STORE}/models/{TIMESTAMP}\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}\n",
    "%env BASE_OUTPUT_DIR={BASE_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b557eb-894c-4f63-9813-ea686c30d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://spectrain_new/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df403156-47be-4e21-8881-82ee35b1e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"covertype_kfp_pipeline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3424e2-5d06-4bdc-9816-a56f630cc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py pipeline_vertex/pipeline_prebuilt.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56451aec-0c72-4818-98a7-b2fb06c0ce7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8893d9-34ff-4ac3-89d3-56d558c9f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "from pipeline_vertex.pipeline_prebuilt import create_pipeline\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_JSON,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07401f96-1092-4cf3-9b9e-7698b4029334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdd575-6de6-4b9a-9a3b-44d0643883df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "481bafe8-5359-4367-bf0f-74f5f872f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-kfp-pipeline-20230616161329?project=469700469475\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/469700469475/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20230616161329\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"covertype_kfp_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939b107-b4e5-4e2b-8de2-f8dfac685b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c68c62-f868-4069-9426-caa1f932f7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
