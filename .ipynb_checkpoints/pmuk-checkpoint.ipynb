{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd356671-2854-4fca-a751-6aa2f3eac98a",
   "metadata": {},
   "source": [
    "## Load needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a94cd2d5-22d3-4cfb-adf6-674cb7e24e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import credentials # NOTE this is for future adaption for MCC deployed solution using creds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219455d1-260c-4821-939f-f06d52845347",
   "metadata": {},
   "source": [
    "### Define the functions to be used for processing and persisting data\n",
    "\n",
    "The get_blobs function is a generator for iterating over the GCS bucket objects\n",
    "\n",
    "The write_png_to_gcs aids with the creation of the PNG files in GCS from the original signals spectra files\n",
    "\n",
    "The write_df_to_gcs is used to create the enriched CSV file from the provided dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af79fbf-a332-4727-bb71-0e831626d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function for iterating over the blobs in a GCS bucket\n",
    "def get_blob(blobs):\n",
    "    for blob in blobs:\n",
    "        yield blob\n",
    "\n",
    "# Function to write the PNG file based on the spectra data plot (300x40) \n",
    "# to a png image file in the provided GCS 'bucket' at the specified 'image_dir' path        \n",
    "def write_png_to_gcs(blob, image_dir, bucket):\n",
    "    print(blob.name)\n",
    "    #NOTE: important to use pyplot instantiation this way to ensure no memory leaks\n",
    "    fig = plt.figure(num=1,figsize=(300,40), clear=True)\n",
    "    data = blob.download_as_bytes()\n",
    "    df = pd.read_csv(io.BytesIO(data), sep='\\s', header=None)\n",
    "    # df=pd.read_csv(blob.name, sep='\\s', header=None)\n",
    "    # filename=filename.split('.')[0]\n",
    "    df.columns=['x_axis', 'y_axis']\n",
    "    ax = fig.add_subplot()\n",
    "    ax.plot(df['x_axis'], df['y_axis'])\n",
    "    ax.axis('off')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format='png')\n",
    "\n",
    "    filename = blob.name.split('.')[0] # remove the suffix/file extension\n",
    "    filename = filename.split('/')[1] # remove the containing directory name from filename\n",
    "\n",
    "    upload_blob = bucket.blob(image_dir+filename+'_nmr.png')\n",
    "    upload_blob.upload_from_file(buf, content_type='image/png', rewind=True)\n",
    "\n",
    "    # plt.close()\n",
    "    buf.close()\n",
    "    del df\n",
    "    # del plt\n",
    "    del data\n",
    "    del upload_blob\n",
    "    \n",
    "    return\n",
    "\n",
    "# Write DataFrame content to the specified 'file_path' in the given GCS 'bucket'\n",
    "def write_df_to_gcs(df, file_path, bucket):\n",
    "    upload_blob = bucket.blob(file_path)\n",
    "    upload_blob.upload_from_string(df.to_csv(), content_type='text/csv')\n",
    "    \n",
    "    del upload_blob\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e8269-65ba-46a6-a81f-fe360c866546",
   "metadata": {},
   "source": [
    "### Data enrichment functions\n",
    "Define the enrichment functions to calculate eGFR and time.TX values for each row of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c534b67-99c3-458f-8961-99b005a8c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Carried over from Jeff's notebook here for pre-processing and feature engg\n",
    "# Define the CKD-EPI equation function\n",
    "def calculate_eGFR(row):\n",
    "    if row['Sex'] == 'male':\n",
    "        kappa = 0.9\n",
    "        alpha = -0.302\n",
    "        beta = 1.0\n",
    "    else:\n",
    "        kappa = 0.7\n",
    "        alpha = -0.241\n",
    "        beta = 1.012\n",
    "\n",
    "    eGFR = 142 * min(row['serum_creatinine'] / kappa, 1)**alpha * \\\n",
    "           max(row['serum_creatinine'] / kappa, 1)**(-1.2) * \\\n",
    "           0.9938**row['Patient.Age.at.Biopsy'] * beta\n",
    "    return eGFR\n",
    "\n",
    "#Calculate the age difference (in years) between the Biopsy and the transplant\n",
    "def calculate_time(row):\n",
    "    return row['Patient.Age.at.Biopsy'] - row['Patient.Age.at.TX']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13861c-5caf-44e8-861e-cfb744989f7e",
   "metadata": {},
   "source": [
    "### Data processing functions\n",
    "The following functions would be used for:  \n",
    "    1. <b>create_input_layers()</b> : Create a dictionary of Keres input layers for each feature  \n",
    "    2. <b>transform(inputs)</b> : Create a dictionary of transformed input tensors   \n",
    "    3. <b>df_to_dataset(dataframe, shuffle, batch_size) </b> : Create a tf.data dataset from a Pandas dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61125520-1436-4d17-854a-57a8322f9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_layers():\n",
    "    \"\"\"Creates dictionary of input layers for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
    "    \"\"\"\n",
    "    inputs = {\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(1,), dtype=\"float32\"\n",
    "        )\n",
    "        for colname in NUMERICAL_COLUMNS\n",
    "    }\n",
    "\n",
    "    inputs.update(\n",
    "        {\n",
    "            colname: tf.keras.layers.Input(\n",
    "                name=colname, shape=(1,), dtype=\"string\"\n",
    "            )\n",
    "            for colname in CATEGORICAL_COLUMNS\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def transform(inputs):\n",
    "    \"\"\"Creates dictionary of transformed inputs.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of transformed Tensors\n",
    "    \"\"\"\n",
    "\n",
    "    transformed = {}\n",
    "    for numerical_column in NUMERICAL_COLUMNS:\n",
    "        transformed[numerical_column] = inputs[numerical_column]\n",
    "\n",
    "    vocab = {\n",
    "        \"Sex\": [\"male\", \"female\", \"unknown\"],\n",
    "        \"Diabetes\": [\"True\", \"False\"],\n",
    "        \"Hypertension\": [\"True\", \"False\"],\n",
    "        \"UA.Pro\": [\"True\", \"False\", \"NaN\"],\n",
    "        \"UA.Hb\": [\"True\", \"False\", \"NaN\"],\n",
    "    }\n",
    "\n",
    "    for categorical_column in CATEGORICAL_COLUMNS:\n",
    "        transformed[categorical_column] = tf.keras.layers.StringLookup(\n",
    "            vocabulary=vocab[categorical_column], output_mode=\"one_hot\"\n",
    "        )(inputs[categorical_column])\n",
    "\n",
    "    return transformed\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"median_house_value\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655390b1-eae9-4625-bb36-eb09616dc846",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: This is commented out as the images (png files) for the spectra are already generated now\n",
    "# image_dir='spec_train_output/images/'\n",
    "\n",
    "# initialize the GCS client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# get the storage bucket\n",
    "bucket = storage_client.get_bucket('spectrain')\n",
    "\n",
    "## NOTE: This is commented out as the images (png files) for the spectra are already generated now\n",
    "# # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "# blobs = storage_client.list_blobs('spectrain', prefix='Kidney_TX_Data')\n",
    "\n",
    "# i = 0 # counter to use for breaking\n",
    "\n",
    "# # Note: The call returns a response only when the iterator is consumed.\n",
    "# for blob in get_blob(blobs):\n",
    "#     if(\"output\" in blob.name):\n",
    "#         write_png_to_gcs(blob, image_dir, bucket)\n",
    "#         # i = i+1\n",
    "#         # if(i == 3):\n",
    "#         #     break;\n",
    "\n",
    "df=pd.read_csv('gs://spectrain/Kidney_TX_Data/Kidney_TX_data.csv')\n",
    "df = df.filter(regex=r'^(?!LS|Banff|Biopsy|Source|Patient.S|Nmr)')\n",
    "\n",
    "# Apply the calculate_eGFR function to create the 'eGFR' column\n",
    "df['eGFR'] = df.apply(calculate_eGFR, axis=1)\n",
    "# Apply the calculate_time function to create the 'time.TX' column\n",
    "df['time.TX'] = df.apply(calculate_time, axis=1)\n",
    "\n",
    "write_df_to_gcs(df, 'Kidney_TX_enriched_data.csv', bucket)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24248d0-0fb3-447a-aec1-e3df1afdfd6c",
   "metadata": {},
   "source": [
    "### Define a Simple LogisticRegression model for our tabular features and label (Case)\n",
    "\n",
    "We will use the dataframe 'df' formed above to form a simple LogisticRegression model and make some\n",
    "predictions and evaluate against the testing data and report that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891aa46c-c4ad-410a-bb2d-ab1b9e80365e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('simple',\n",
       "                                                  SimpleImputer(strategy='median'),\n",
       "                                                  ['serum_creatinine',\n",
       "                                                   'hippurate',\n",
       "                                                   'phenylacetylglutamine',\n",
       "                                                   'trigonellin', 'urea',\n",
       "                                                   'citrate', 'dimethylamine',\n",
       "                                                   'lactate', 'eGFR',\n",
       "                                                   'time.TX']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(sparse=False),\n",
       "                                                  ['Sex', 'Diabetes',\n",
       "                                                   'Hypertension', 'UA.Pro',\n",
       "                                                   'UA.Hb']),\n",
       "                                                 ('scale', StandardScaler(),\n",
       "                                                  ['serum_creatinine',\n",
       "                                                   'hippurate',\n",
       "                                                   'phenylacetylglutamine',\n",
       "                                                   'trigonellin', 'urea',\n",
       "                                                   'citrate', 'dimethylamine',\n",
       "                                                   'lactate', 'eGFR',\n",
       "                                                   'time.TX'])])),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(max_iter=10000, random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('gs://spectrain/Kidney_TX_enriched_data.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "X = df[[\"serum_creatinine\",\"Sex\",\"hippurate\",\"phenylacetylglutamine\",\n",
    "        \"trigonellin\",\"urea\",\"citrate\",\"dimethylamine\",\"lactate\",\n",
    "        \"Diabetes\",\"Hypertension\",\"UA.Pro\",\"UA.Hb\",\"eGFR\",\"time.TX\"]]\n",
    "y = df[\"Case\"]\n",
    "\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"serum_creatinine\", \"hippurate\", \"phenylacetylglutamine\", \"trigonellin\",\n",
    "                    \"urea\", \"citrate\", \"dimethylamine\", \"lactate\", \"eGFR\", \"time.TX\"]\n",
    "CATEGORICAL_COLUMNS = [\"Sex\", \"Diabetes\", \"Hypertension\", \"UA.Pro\", \"UA.Hb\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "# get the categorical and numeric column names\n",
    "num_cols = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# pipeline for numerical columns\n",
    "num_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# # pipeline for categorical columns\n",
    "# cat_pipe = make_pipeline(\n",
    "#     SimpleImputer(strategy='constant', fill_value='N/A'),\n",
    "#     OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# )\n",
    "\n",
    "# combine both the pipelines\n",
    "# full_pipe = ColumnTransformer([\n",
    "#     ('num', num_pipe, num_cols),\n",
    "#     ('cat', cat_pipe, cat_cols)\n",
    "# ])\n",
    "\n",
    "# # Create input layer\n",
    "# inputs = create_input_layers()\n",
    "\n",
    "# # transform\n",
    "# transformed = transform(inputs)\n",
    "\n",
    "logreg = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"simple\", SimpleImputer(strategy='median'), NUMERICAL_COLUMNS),\n",
    "            (\"ohe\", OneHotEncoder(sparse=False), CATEGORICAL_COLUMNS),\n",
    "            (\"scale\", StandardScaler(with_mean=True), NUMERICAL_COLUMNS),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    LogisticRegression(max_iter=10000, random_state=42),\n",
    ")\n",
    "\n",
    "# train the model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae34d0-7da0-4b1e-ae17-4aad627605d7",
   "metadata": {},
   "source": [
    "Now predict the outcomes for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99e0569-841c-4d22-ad4b-3bf7e1806122",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5c271-dcc4-4f11-985b-f2cc3338b483",
   "metadata": {},
   "source": [
    "Finally, show the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e334225c-0924-4e70-87a6-01bb4e24f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83       197\n",
      "           1       0.67      0.17      0.27        84\n",
      "\n",
      "    accuracy                           0.73       281\n",
      "   macro avg       0.70      0.57      0.55       281\n",
      "weighted avg       0.71      0.73      0.66       281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f1526-cfe2-4472-bf66-e69bd32074a4",
   "metadata": {},
   "source": [
    "### Trying a Keras based deep learning model with a neural network of 3 layers\n",
    "\n",
    "First dense layer of 16, second dense layer of 8 and the last layer using sigmoid activation. \n",
    "The first 2 dense layers would use ReLU as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04fa8e2-9d1b-4351-86e2-8c5db83ac38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "188/188 [==============================] - 1s 1ms/step - loss: 2.8546 - auc: 0.5171\n",
      "Epoch 2/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.8363 - auc: 0.5677\n",
      "Epoch 3/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6386 - auc: 0.6083\n",
      "Epoch 4/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6258 - auc: 0.6102\n",
      "Epoch 5/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6188 - auc: 0.6204\n",
      "Epoch 6/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6181 - auc: 0.6033\n",
      "Epoch 7/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6070 - auc: 0.6185\n",
      "Epoch 8/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6050 - auc: 0.6220\n",
      "Epoch 9/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6019 - auc: 0.6202\n",
      "Epoch 10/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5961 - auc: 0.6144\n",
      "Epoch 11/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.6010 - auc: 0.6203\n",
      "Epoch 12/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5826 - auc: 0.6539\n",
      "Epoch 13/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5918 - auc: 0.6325\n",
      "Epoch 14/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5927 - auc: 0.6272\n",
      "Epoch 15/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5815 - auc: 0.6466\n",
      "Epoch 16/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5886 - auc: 0.6210\n",
      "Epoch 17/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5919 - auc: 0.6257\n",
      "Epoch 18/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5877 - auc: 0.6354\n",
      "Epoch 19/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5809 - auc: 0.6479\n",
      "Epoch 20/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5837 - auc: 0.6403\n",
      "Epoch 21/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5846 - auc: 0.6324\n",
      "Epoch 22/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5848 - auc: 0.6548\n",
      "Epoch 23/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5803 - auc: 0.6409\n",
      "Epoch 24/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5783 - auc: 0.6459\n",
      "Epoch 25/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5813 - auc: 0.6470\n",
      "Epoch 26/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5920 - auc: 0.6383\n",
      "Epoch 27/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5838 - auc: 0.6479\n",
      "Epoch 28/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5805 - auc: 0.6392\n",
      "Epoch 29/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5777 - auc: 0.6496\n",
      "Epoch 30/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5797 - auc: 0.6457\n",
      "Epoch 31/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5757 - auc: 0.6494\n",
      "Epoch 32/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5922 - auc: 0.6336\n",
      "Epoch 33/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5841 - auc: 0.6424\n",
      "Epoch 34/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5782 - auc: 0.6482\n",
      "Epoch 35/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5743 - auc: 0.6544\n",
      "Epoch 36/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5726 - auc: 0.6565\n",
      "Epoch 37/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5765 - auc: 0.6534\n",
      "Epoch 38/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5709 - auc: 0.6605\n",
      "Epoch 39/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5909 - auc: 0.6348\n",
      "Epoch 40/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5794 - auc: 0.6391\n",
      "Epoch 41/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5746 - auc: 0.6543\n",
      "Epoch 42/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5739 - auc: 0.6584\n",
      "Epoch 43/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5727 - auc: 0.6578\n",
      "Epoch 44/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5701 - auc: 0.6615\n",
      "Epoch 45/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5672 - auc: 0.6711\n",
      "Epoch 46/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5678 - auc: 0.6659\n",
      "Epoch 47/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5742 - auc: 0.6556\n",
      "Epoch 48/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5784 - auc: 0.6563\n",
      "Epoch 49/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5686 - auc: 0.6601\n",
      "Epoch 50/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5794 - auc: 0.6534\n",
      "Epoch 51/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5704 - auc: 0.6594\n",
      "Epoch 52/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5739 - auc: 0.6581\n",
      "Epoch 53/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5689 - auc: 0.6629\n",
      "Epoch 54/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5689 - auc: 0.6615\n",
      "Epoch 55/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5684 - auc: 0.6649\n",
      "Epoch 56/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5744 - auc: 0.6583\n",
      "Epoch 57/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5691 - auc: 0.6634\n",
      "Epoch 58/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5711 - auc: 0.6635\n",
      "Epoch 59/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5721 - auc: 0.6612\n",
      "Epoch 60/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5860 - auc: 0.6468\n",
      "Epoch 61/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5647 - auc: 0.6714\n",
      "Epoch 62/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5711 - auc: 0.6678\n",
      "Epoch 63/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5655 - auc: 0.6691\n",
      "Epoch 64/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5664 - auc: 0.6692\n",
      "Epoch 65/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5674 - auc: 0.6642\n",
      "Epoch 66/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5663 - auc: 0.6679\n",
      "Epoch 67/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5651 - auc: 0.6669\n",
      "Epoch 68/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5713 - auc: 0.6582\n",
      "Epoch 69/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5654 - auc: 0.6693\n",
      "Epoch 70/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5669 - auc: 0.6662\n",
      "Epoch 71/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5693 - auc: 0.6700\n",
      "Epoch 72/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5623 - auc: 0.6768\n",
      "Epoch 73/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5596 - auc: 0.6822\n",
      "Epoch 74/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5638 - auc: 0.6751\n",
      "Epoch 75/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5675 - auc: 0.6653\n",
      "Epoch 76/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5596 - auc: 0.6801\n",
      "Epoch 77/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5623 - auc: 0.6866\n",
      "Epoch 78/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5587 - auc: 0.6787\n",
      "Epoch 79/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5636 - auc: 0.6807\n",
      "Epoch 80/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5603 - auc: 0.6783\n",
      "Epoch 81/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5628 - auc: 0.6771\n",
      "Epoch 82/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5609 - auc: 0.6753\n",
      "Epoch 83/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5609 - auc: 0.6793\n",
      "Epoch 84/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5601 - auc: 0.6803\n",
      "Epoch 85/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5694 - auc: 0.6698\n",
      "Epoch 86/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5580 - auc: 0.6806\n",
      "Epoch 87/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5627 - auc: 0.6836\n",
      "Epoch 88/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5571 - auc: 0.6801\n",
      "Epoch 89/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5578 - auc: 0.6859\n",
      "Epoch 90/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5571 - auc: 0.6826\n",
      "Epoch 91/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5561 - auc: 0.6879\n",
      "Epoch 92/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5619 - auc: 0.6808\n",
      "Epoch 93/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5492 - auc: 0.7016\n",
      "Epoch 94/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5737 - auc: 0.6783\n",
      "Epoch 95/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5663 - auc: 0.6749\n",
      "Epoch 96/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5530 - auc: 0.6934\n",
      "Epoch 97/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5570 - auc: 0.6836\n",
      "Epoch 98/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5565 - auc: 0.6846\n",
      "Epoch 99/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5568 - auc: 0.6798\n",
      "Epoch 100/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5574 - auc: 0.6837\n",
      "Epoch 101/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5549 - auc: 0.6838\n",
      "Epoch 102/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5555 - auc: 0.6861\n",
      "Epoch 103/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5515 - auc: 0.6927\n",
      "Epoch 104/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5537 - auc: 0.6875\n",
      "Epoch 105/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5641 - auc: 0.6904\n",
      "Epoch 106/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5617 - auc: 0.6799\n",
      "Epoch 107/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5528 - auc: 0.6915\n",
      "Epoch 108/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5518 - auc: 0.6927\n",
      "Epoch 109/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5795 - auc: 0.6841\n",
      "Epoch 110/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5520 - auc: 0.6906\n",
      "Epoch 111/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5592 - auc: 0.6865\n",
      "Epoch 112/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5585 - auc: 0.6868\n",
      "Epoch 113/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5562 - auc: 0.6898\n",
      "Epoch 114/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5513 - auc: 0.6938\n",
      "Epoch 115/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5533 - auc: 0.6888\n",
      "Epoch 116/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5522 - auc: 0.6884\n",
      "Epoch 117/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5568 - auc: 0.6786\n",
      "Epoch 118/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5534 - auc: 0.6910\n",
      "Epoch 119/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5537 - auc: 0.6883\n",
      "Epoch 120/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5545 - auc: 0.6870\n",
      "Epoch 121/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5463 - auc: 0.6985\n",
      "Epoch 122/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5487 - auc: 0.6954\n",
      "Epoch 123/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5489 - auc: 0.6958\n",
      "Epoch 124/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5511 - auc: 0.6890\n",
      "Epoch 125/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5518 - auc: 0.6918\n",
      "Epoch 126/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5454 - auc: 0.7033\n",
      "Epoch 127/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5576 - auc: 0.6920\n",
      "Epoch 128/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5579 - auc: 0.6772\n",
      "Epoch 129/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5571 - auc: 0.6799\n",
      "Epoch 130/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5502 - auc: 0.6960\n",
      "Epoch 131/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5466 - auc: 0.6974\n",
      "Epoch 132/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5569 - auc: 0.6844\n",
      "Epoch 133/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5494 - auc: 0.6956\n",
      "Epoch 134/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5565 - auc: 0.6856\n",
      "Epoch 135/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5520 - auc: 0.6896\n",
      "Epoch 136/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5464 - auc: 0.6943\n",
      "Epoch 137/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5490 - auc: 0.6967\n",
      "Epoch 138/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5464 - auc: 0.7009\n",
      "Epoch 139/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5514 - auc: 0.6875\n",
      "Epoch 140/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5485 - auc: 0.6904\n",
      "Epoch 141/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5453 - auc: 0.6999\n",
      "Epoch 142/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5490 - auc: 0.6956\n",
      "Epoch 143/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5525 - auc: 0.6896\n",
      "Epoch 144/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5514 - auc: 0.6842\n",
      "Epoch 145/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5459 - auc: 0.7003\n",
      "Epoch 146/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5437 - auc: 0.7024\n",
      "Epoch 147/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5436 - auc: 0.7043\n",
      "Epoch 148/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5466 - auc: 0.6941\n",
      "Epoch 149/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5457 - auc: 0.7009\n",
      "Epoch 150/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5447 - auc: 0.6979\n",
      "Epoch 151/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5487 - auc: 0.6969\n",
      "Epoch 152/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5456 - auc: 0.7020\n",
      "Epoch 153/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5455 - auc: 0.6997\n",
      "Epoch 154/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5450 - auc: 0.7027\n",
      "Epoch 155/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5470 - auc: 0.7056\n",
      "Epoch 156/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5444 - auc: 0.6991\n",
      "Epoch 157/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5473 - auc: 0.6997\n",
      "Epoch 158/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5419 - auc: 0.7068\n",
      "Epoch 159/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5398 - auc: 0.7042\n",
      "Epoch 160/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5437 - auc: 0.7006\n",
      "Epoch 161/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5436 - auc: 0.7013\n",
      "Epoch 162/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5435 - auc: 0.7012\n",
      "Epoch 163/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5445 - auc: 0.6976\n",
      "Epoch 164/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5524 - auc: 0.6934\n",
      "Epoch 165/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5463 - auc: 0.6991\n",
      "Epoch 166/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5390 - auc: 0.7071\n",
      "Epoch 167/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5389 - auc: 0.7054\n",
      "Epoch 168/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5399 - auc: 0.7054\n",
      "Epoch 169/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5372 - auc: 0.7104\n",
      "Epoch 170/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5396 - auc: 0.7083\n",
      "Epoch 171/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5478 - auc: 0.6979\n",
      "Epoch 172/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5365 - auc: 0.7158\n",
      "Epoch 173/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5488 - auc: 0.6969\n",
      "Epoch 174/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5401 - auc: 0.7102\n",
      "Epoch 175/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5433 - auc: 0.6984\n",
      "Epoch 176/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5426 - auc: 0.7036\n",
      "Epoch 177/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5408 - auc: 0.7069\n",
      "Epoch 178/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5464 - auc: 0.6976\n",
      "Epoch 179/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5405 - auc: 0.7045\n",
      "Epoch 180/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5428 - auc: 0.7029\n",
      "Epoch 181/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5353 - auc: 0.7184\n",
      "Epoch 182/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5387 - auc: 0.7099\n",
      "Epoch 183/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5405 - auc: 0.7115\n",
      "Epoch 184/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5414 - auc: 0.7064\n",
      "Epoch 185/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5397 - auc: 0.7041\n",
      "Epoch 186/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5367 - auc: 0.7139\n",
      "Epoch 187/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5396 - auc: 0.7087\n",
      "Epoch 188/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5379 - auc: 0.7089\n",
      "Epoch 189/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5372 - auc: 0.7122\n",
      "Epoch 190/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5399 - auc: 0.7038\n",
      "Epoch 191/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5379 - auc: 0.7125\n",
      "Epoch 192/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5360 - auc: 0.7127\n",
      "Epoch 193/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5396 - auc: 0.7075\n",
      "Epoch 194/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5450 - auc: 0.6971\n",
      "Epoch 195/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5390 - auc: 0.7093\n",
      "Epoch 196/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5350 - auc: 0.7149\n",
      "Epoch 197/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5398 - auc: 0.7043\n",
      "Epoch 198/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5401 - auc: 0.7039\n",
      "Epoch 199/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5394 - auc: 0.7108\n",
      "Epoch 200/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5395 - auc: 0.7101\n",
      "Epoch 201/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5341 - auc: 0.7185\n",
      "Epoch 202/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5363 - auc: 0.7134\n",
      "Epoch 203/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5385 - auc: 0.7081\n",
      "Epoch 204/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5350 - auc: 0.7140\n",
      "Epoch 205/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5341 - auc: 0.7171\n",
      "Epoch 206/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5328 - auc: 0.7221\n",
      "Epoch 207/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5370 - auc: 0.7134\n",
      "Epoch 208/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5389 - auc: 0.7098\n",
      "Epoch 209/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5383 - auc: 0.7118\n",
      "Epoch 210/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5381 - auc: 0.7094\n",
      "Epoch 211/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5328 - auc: 0.7183\n",
      "Epoch 212/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5371 - auc: 0.7148\n",
      "Epoch 213/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5338 - auc: 0.7173\n",
      "Epoch 214/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5366 - auc: 0.7107\n",
      "Epoch 215/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5351 - auc: 0.7146\n",
      "Epoch 216/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5324 - auc: 0.7195\n",
      "Epoch 217/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5319 - auc: 0.7235\n",
      "Epoch 218/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5309 - auc: 0.7249\n",
      "Epoch 219/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5351 - auc: 0.7235\n",
      "Epoch 220/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5366 - auc: 0.7122\n",
      "Epoch 221/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5315 - auc: 0.7242\n",
      "Epoch 222/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5331 - auc: 0.7178\n",
      "Epoch 223/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5365 - auc: 0.7120\n",
      "Epoch 224/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5329 - auc: 0.7183\n",
      "Epoch 225/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5356 - auc: 0.7132\n",
      "Epoch 226/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5343 - auc: 0.7198\n",
      "Epoch 227/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5317 - auc: 0.7235\n",
      "Epoch 228/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5353 - auc: 0.7203\n",
      "Epoch 229/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5327 - auc: 0.7204\n",
      "Epoch 230/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5309 - auc: 0.7244\n",
      "Epoch 231/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5391 - auc: 0.7128\n",
      "Epoch 232/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5333 - auc: 0.7207\n",
      "Epoch 233/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5299 - auc: 0.7209\n",
      "Epoch 234/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5321 - auc: 0.7167\n",
      "Epoch 235/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5317 - auc: 0.7179\n",
      "Epoch 236/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5280 - auc: 0.7271\n",
      "Epoch 237/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5348 - auc: 0.7195\n",
      "Epoch 238/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5327 - auc: 0.7194\n",
      "Epoch 239/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5303 - auc: 0.7202\n",
      "Epoch 240/240\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.5309 - auc: 0.7257\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6235 - auc: 0.6414\n",
      "Test Loss: 0.6235335469245911 and test AUC: 0.6413766145706177\n"
     ]
    }
   ],
   "source": [
    "#re-initialize the dataframe\n",
    "df = pd.read_csv('gs://spectrain/Kidney_TX_enriched_data.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "#TODO Try Dense layers with more neurons (200s) per suggestion from Benoit\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(15,)),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "\ttf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "\t# tf.keras.layers.Dense(4, activation=tf.nn.relu), # added one after the 3 layer\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "])\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "# NOTE: cannot use the following construct as unable to use .fit with support for epochs and batch_size params\n",
    "# dnn = make_pipeline(\n",
    "#     ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"simple\", SimpleImputer(strategy='median'), NUMERICAL_COLUMNS),\n",
    "#             (\"ohe\", OneHotEncoder(sparse=False), CATEGORICAL_COLUMNS),\n",
    "#             (\"scale\", StandardScaler(with_mean=True), NUMERICAL_COLUMNS),\n",
    "#         ],\n",
    "#         remainder=\"passthrough\",\n",
    "#     ),\n",
    "#     model,\n",
    "# )\n",
    "\n",
    "# [print(i.shape, i.dtype) for i in model.inputs]\n",
    "# [print(o.shape, o.dtype) for o in model.outputs]\n",
    "# [print(l.name, l.input_shape, l.dtype) for l in model.layers]\n",
    "\n",
    "data_dummy = pd.get_dummies(df[CATEGORICAL_COLUMNS], drop_first=True)\n",
    "\n",
    "df = pd.concat([df, data_dummy], axis=1)\n",
    "\n",
    "# drop the original categorical columns that have been one hot encoded\n",
    "df = df.drop(CATEGORICAL_COLUMNS, axis=1)\n",
    "\n",
    "# df = df.dropna()\n",
    "\n",
    "X = df[[\"serum_creatinine\",\"Sex_male\",\"hippurate\",\"phenylacetylglutamine\",\n",
    "        \"trigonellin\",\"urea\",\"citrate\",\"dimethylamine\",\"lactate\",\n",
    "        \"Diabetes_True\",\"Hypertension_True\",\"UA.Pro_True\",\"UA.Hb_True\",\"eGFR\",\"time.TX\"]]\n",
    "y = df[\"Case\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# train_data = dataset.shuffle(len(X_train)).batch(32)\n",
    "# train_data = train_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "# valid_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "# model.fit(train_data, epochs=200, batch_size=4, validation_data=valid_ds)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=240, batch_size=6)\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Loss: {test_loss} and test accuracy: {test_acc}\")\n",
    "\n",
    "# NOTE: For printing loss and AUC\n",
    "test_loss, test_auc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss} and test AUC: {test_auc}\")\n",
    "\n",
    "# Sample output with 1000 epochs and 32 batch size (0.91 for the loss is less than ideal :( )\n",
    "# Test Loss: 0.9106149673461914 and test accuracy: 0.7298578023910522\n",
    "\n",
    "#TODO perhaps try loss, mse instead of loss, accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f0290-d2ad-4daa-80ba-0f5e5b2a1281",
   "metadata": {},
   "source": [
    "### Validation results\n",
    "\n",
    "With 240 epochs and 3 layers of 16, 8 and 1 (sigmoid) and batch size 8   \n",
    "Epoch 240/240   \n",
    "145/145 [==============================] - 0s 1ms/step - loss: 0.5399 - accuracy: 0.7476   \n",
    "10/10 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7266   \n",
    "Test Loss: 0.5679342746734619 and test accuracy: 0.7266436219215393   \n",
    "   \n",
    "   \n",
    "With 240 epochs and 4 layers of 16, 8, 4 and 1 (sigmoid) and batch size 8   \n",
    "Epoch 240/240   \n",
    "145/145 [==============================] - 0s 1ms/step - loss: 0.5415 - accuracy: 0.7415   \n",
    "10/10 [==============================] - 0s 1ms/step - loss: 0.5757 - accuracy: 0.7370   \n",
    "Test Loss: 0.5757291316986084 and test accuracy: 0.7370242476463318   \n",
    "\n",
    "With 240 epochs and 4 layers of 16, 8, 4 and 1 (sigmoid) and batch size 6   \n",
    "Epoch 240/240   \n",
    "193/193 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7476   \n",
    "10/10 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.7405   \n",
    "Test Loss: 0.5814240574836731 and test accuracy: 0.7404844164848328\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1649e-fb5f-461e-a6f3-6d57e4fad89e",
   "metadata": {},
   "source": [
    "### Upload the CSV based DNN model and deploy to Vertex AI  \n",
    "\n",
    "Upload the DNN model created for the hyperparameter tuning in the 'spectrain_new/spectrain_csv_dnn/tuned_20230615_175922' GCS path and deploy it to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f90e08-aafb-4e45-90bb-aa132f0faed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwiklabs-asl-00-c812c3b423f2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT = PROJECT[0]\n",
    "PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff39bb85-6b23-4c2f-864d-7ee074fab8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/469700469475/locations/us-central1/models/1914199166423138304/operations/1252336253613899776\n",
      "Model created. Resource name: projects/469700469475/locations/us-central1/models/1914199166423138304@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/469700469475/locations/us-central1/models/1914199166423138304@1')\n"
     ]
    }
   ],
   "source": [
    "REGION = \"us-central1\"\n",
    "BUCKET = \"spectrain_new\"\n",
    "\n",
    "MODEL_DISPLAYNAME = \"spectrain_csv_dnn/tuned_20230615_175922/20230615181110\"\n",
    "# us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    ")\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "\n",
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\", # TODO: Your code here\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI  # TODO: Your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc2336-0a46-435d-8473-d7e8f65edf77",
   "metadata": {},
   "source": [
    "### Deploy an endpoint for the uploaded model with a n1-standard-2 machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db160a8e-6ca0-4f53-8f0a-0476f3a0ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/469700469475/locations/us-central1/endpoints/8892177144336089088/operations/5699640885642264576\n",
      "Endpoint created. Resource name: projects/469700469475/locations/us-central1/endpoints/8892177144336089088\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/469700469475/locations/us-central1/endpoints/8892177144336089088')\n",
      "Deploying model to Endpoint : projects/469700469475/locations/us-central1/endpoints/8892177144336089088\n",
      "Deploy Endpoint model backing LRO: projects/469700469475/locations/us-central1/endpoints/8892177144336089088/operations/3074042302885265408\n",
      "Endpoint model deployed. Resource name: projects/469700469475/locations/us-central1/endpoints/8892177144336089088\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard-2\"\n",
    "\n",
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=MACHINE_TYPE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec075e-0f72-41cc-b955-cded477304b2",
   "metadata": {},
   "source": [
    "### Upload the images based CNN model and deploy to Vertex AI\n",
    "\n",
    "NOTE: We need to replace the GCS path value with the right one for the CNN model  \n",
    "\n",
    "Upload the CNN model created for the hyperparameter tuning in the 'spectrain_new/<REST_OF_PATH>' GCS path and deploy it to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcce774-6c42-48aa-907d-6d5049dfdadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/469700469475/locations/us-central1/models/4203153677034192896/operations/4600762576563863552\n",
      "Model created. Resource name: projects/469700469475/locations/us-central1/models/4203153677034192896@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/469700469475/locations/us-central1/models/4203153677034192896@1')\n"
     ]
    }
   ],
   "source": [
    "REGION = \"us-central1\"\n",
    "BUCKET = \"spectrain_new\"\n",
    "\n",
    "MODEL_DISPLAYNAME = \"spectrain_cnn\" # TODO\n",
    "\n",
    "# us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
    ")\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "\n",
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAYNAME,\n",
    "    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\", # TODO: Your code here\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI  # TODO: Your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845d92c-ce86-4912-b2b7-3c4665227986",
   "metadata": {},
   "source": [
    "### Process all test images into a single JSON for batch prediction\n",
    "\n",
    "Take the test image files from the 'bhavani/transformed_images' folder in the GCS bucket and write the contents as base64 encoded data to the JSON file used for batch predictions  \n",
    "  \n",
    "Name of batch prediction 'spectrain_cnn_batch_test' (against the test dataset images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b28565c7-02fc-4669-b694-382f49927629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_file(blob, f):\n",
    "#     blob_data = blob.download_as_bytes()\n",
    "    \n",
    "#     data = {\"data\": base64.b64encode(blob_data).decode(\"utf-8\")}\n",
    "#     f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "def write_file_to_gcs(image_file, f):\n",
    "    # initialize the GCS client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # get the storage bucket\n",
    "    bucket = storage_client.get_bucket('spectrain')\n",
    "\n",
    "    # NOTE: This is commented out as the images (png files) for the spectra are already generated now\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    # spectrain_new/bhavani/transformed_images\n",
    "    blobs = storage_client.list_blobs('spectrain_new', prefix='bhavani/transformed_images')\n",
    "\n",
    "    i = 0 # counter to use for breaking\n",
    "\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "    for blob in get_blob(blobs):\n",
    "        blobname = blob.name.split('.')[0]\n",
    "        blobname = blobname.split('/')[2]\n",
    "        \n",
    "        if(blobname == image_file):\n",
    "            print(f\"Writing image_file : {image_file} to JSON\")\n",
    "            blob_data = blob.download_as_bytes()\n",
    "    \n",
    "            data = {\"data\": base64.b64encode(blob_data).decode(\"utf-8\")}\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "            # write_file(blob, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7eebb9-056e-4d22-aa3f-4f721c511286",
   "metadata": {},
   "source": [
    "### Write JSONL file with content as the list of GCS paths to a single image per line\n",
    "\n",
    "Adjust and createt new function to make JSONL file with the list of the images as specified in the documentation here:  \n",
    "https://cloud.google.com/vertex-ai/docs/image-data/classification/get-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2400fcb5-a134-472c-9f82-1a6b13961d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_filepath_to_gcs(image_file, f):\n",
    "    # initialize the GCS client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # get the storage bucket\n",
    "    bucket = storage_client.get_bucket('spectrain')\n",
    "\n",
    "    # NOTE: This is commented out as the images (png files) for the spectra are already generated now\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    # spectrain_new/bhavani/transformed_images\n",
    "    blobs = storage_client.list_blobs('spectrain_new', prefix='bhavani/transformed_images')\n",
    "\n",
    "    i = 0 # counter to use for breaking\n",
    "\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "    for blob in get_blob(blobs):\n",
    "        blobname = blob.name.split('.')[0]\n",
    "        blobname = blobname.split('/')[2]\n",
    "        \n",
    "        if(blobname == image_file):\n",
    "            print(f\"Writing image_file path : {blob} to JSON\")\n",
    "            # blob_data = blob.download_as_bytes()\n",
    "    \n",
    "            data = {\"content\": \"gs://spectrain_new/\"+blob.name, \"mimeType\": \"image/png\"}\n",
    "            # print(json.dumps(data))\n",
    "            f.write(json.dumps(data) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d264892-e99f-4185-b680-c8fe8a37ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location to use for CNN images JSON for batch predictions\n",
    "# spectrain_new/bhavani/batch_predictions/IMG_CNN\n",
    "\n",
    "# NOTE: use the transformed_images folder in 'bhavani'\n",
    "# spectrain_new/bhavani/transformed_images\n",
    "\n",
    "\n",
    "## IMP NOTE: Commenting this block as the JSON file for the test images was already \n",
    "# created by the execution of the underlying code\n",
    "# NOTE determine the test images from the _split csv file\n",
    "# spectrain_new/Kidney_TX_data_with_split.csv\n",
    "# df1=pd.read_csv('gs://spectrain_new/Kidney_TX_data_with_split.csv')\n",
    "\n",
    "# df1= df1[[\"Spectrum_file\",\"data_split\"]]\n",
    "\n",
    "# df1 = df1.loc[df1[\"data_split\"] == \"TEST\"]\n",
    "\n",
    "# gcs_input_uri = \"gs://spectrain_new/bhavani/batch_predictions/IMG_CNN/src/test_images_new.json\"\n",
    "\n",
    "# with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
    "#     for fl in df1[\"Spectrum_file\"]:\n",
    "#         fl_name = fl.split('.')[0] + \"_nmr\"\n",
    "#         write_filepath_to_gcs(fl_name,f)\n",
    "#         # break\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160e21c-c977-4a5d-bd64-2d805d970729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
