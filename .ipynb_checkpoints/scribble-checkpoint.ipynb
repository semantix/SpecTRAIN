{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900530fe-4b5d-41b4-8820-3f81c9db4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"gs://spectrain/bhavani/canny_images/output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_12_571_00000_withoutBackground_20001_nmr.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18af8968-fc41-4569-ba51-b74c0968e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://spectrain/bhavani/canny_images/output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_12_573_00000_withoutBackground_20001_nmr_nmr.png...\n",
      "/ [1 files][100.2 KiB/100.2 KiB]                                                \n",
      "Operation completed over 1 objects/100.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://spectrain/bhavani/canny_images/output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_12_573_00000_withoutBackground_20001_nmr_nmr.png ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a95b60ce-c210-44d6-bc18-589ee0e354f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "image = tf.io.read_file(\"output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_12_572_00000_withoutBackground_20001_nmr_nmr.png\")\n",
    "image = tf.io.decode_png(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7bb6e0a4-ce44-414d-a077-c5a3beffae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('gs://spectrain/spec_train_output/image_dir_paths_labels_with_split.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "92ce874a-2c04-4268-9c7d-42a697b1511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['split', 'directory', 'label']\n",
    "df[\"proc_img_dir\"] = \"gs://spectrain/bhavani/transformed_images/\"+train_df.directory.str.split('/').str[-1]\n",
    "train_df = df[df.split=='TRAINING']\n",
    "valid_df = df[df.split=='VALIDATION']\n",
    "test_df = df[df.split=='TEST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bc47240f-f43c-4845-a428-fd5410d205c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5f325679-d783-4b75-85ad-b8b0b27ab6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=train_df[['label','proc_img_dir']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c0981fc-6544-4110-a321-37b91d16f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.groupby(['split', 'label'])['directory'].apply(list).reset_index(name='file_paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5dd46920-949e-4db8-a11e-18c62087c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: The mb command requires a URL that specifies a bucket.\n",
      "\"gs://spectrain/bhavani/train_images/pos\" is not valid.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mkdir gs://spectrain/bhavani/train_images/pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fdd0edd2-68bf-49f0-85b3-c62797e57fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f6315896-25e2-4365-9cea-75e497f2ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_files=train_df[train_df.label==0].proc_img_dir.values.tolist()\n",
    "pos_files=train_df[train_df.label==1].proc_img_dir.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3a1f9a04-c701-4df3-b771-6490c32e311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=pos_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eaaf7593-f9ed-4eb8-bcbf-3d12786bc518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://spectrain/bhavani/transformed_images/output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_15_770_00000_withoutBackground_20001_nmr.png'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a5a83557-5e72-43a0-adf1-9f09aaeea892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_15_770_00000_withoutBackground_20001_nmr.png\n"
     ]
    }
   ],
   "source": [
    "filename = file1.split('/')[-1]\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d95ffe05-84dc-4564-b3a2-5adee711dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://spectrain/bhavani/transformed_images/output_NormalizationTool_spectrum_zgpr30-urine-600MHz-310K_15_770_00000_withoutBackground_20001_nmr.png [Content-Type=image/png]...\n",
      "/ [1 files][106.9 KiB/106.9 KiB]                                                \n",
      "Operation completed over 1 objects/106.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# remove the containing directory name from filename\n",
    "split_name='train'\n",
    "label='positive'\n",
    "!gsutil cp {file1} gs://spectrain/bhavani/{split_name}_images/{label}/{filename}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4e348c1-a4ca-4e24-b7fd-907973876b99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5b440388-a850-46f2-8c3a-6cc3c8afcdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 748)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df.label==1]), len(train_df[train_df.label==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2cb9eb0c-847e-4916-a7f7-69d296fe365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 56)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df[valid_df.label==0]), len(valid_df[valid_df.label==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4beb258d-c7c2-4200-8218-fcf02188c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df[test_df.label==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "930366ed-0bde-44d2-9fde-0c5379f24e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222, 1064, 188)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df), len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65b7da-0490-408a-aaed-98762b6309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an init file to identify the following code as a package\n",
    "%%bash\n",
    "mkdir -p babyweight/trainer\n",
    "touch babyweight/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa370a-6431-4312-9dc9-269744cd11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file to parse the arguments\n",
    "# We will use this later to parse arguments when training the model\n",
    "%%writefile babyweight/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        default = os.getenv(\"AIP_MODEL_DIR\")\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=\"128 32 4\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nembeds\",\n",
    "        help=\"Embedding size of a cross of n key real-valued parameters\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37e5a2-1574-4af8-87b0-5015b2ed95a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place all the preprocessing, model building, training and evaluation code in this cell to package in\n",
    "# model.py to later train directly in vertex ai\n",
    "%%writefile babyweight/trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "\n",
    "def build_model(filter_size_1, filter_size_2, kernel_size, pool_kernel_size, hidden_units_1, hidden_units_2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu', input_shape=(128, 128)))\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size))\n",
    "    model.add(Conv1D(filter_size_2, kernel_size=kernel_size,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Softmax())\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_data(images_dir, batch_size, height, weight, training):\n",
    "    ds=image_dataset_from_directory(images_dir, batch_size, image_size=(height, width), shuffle=True, \n",
    "                                                                        seed=1)\n",
    "   \n",
    "    if training:\n",
    "        return ds.repeat()\n",
    "    else:\n",
    "        return ds\n",
    "    \n",
    "\n",
    "        \n",
    "def train_and_evaluate(train_data_path, valid_data_path, output_dir, batch_size, num_epochs, \n",
    "                       steps_per_epoch, train_examples):\n",
    "    model = build_model()\n",
    "\n",
    "    trainds = load_dataset(train_data_path, batch_size)\n",
    "\n",
    "    evalds = load_dataset(valid_data_path, batch_size, training=False)\n",
    "\n",
    "    num_batches = batch_size * num_epochs\n",
    "    steps_per_epoch = train_examples // num_batches\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2)\n",
    "\n",
    "     model.save(output_dir) # save trained model\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9e7a6-9069-4ce2-acff-aac27600a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2b912-39d1-4cc2-9251-6751da17f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train the model locally to check if everything is good #########\n",
    "%%bash\n",
    "OUTDIR=babyweight_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python3 -m trainer.task \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=10 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701f1ef-d854-45ef-8362-e0a4de9fb78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8d7b2557-d3ee-4040-a670-c8a9c0415474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The following is the code to train the model on vertex ai with a randomly selected hyperparameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449428e1-fac2-4ae3-95ce-46478f000fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a package source distribution\n",
    "%%writefile babyweight/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='babyweight_trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Babyweight model training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d082796-bcee-4c2d-9247-b73124e6d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local directory to store source distribution package\n",
    "%%bash\n",
    "cd babyweight\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca287b2-e2b2-4cc1-b45e-8216101a9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the package in the cloud bucket\n",
    "%%bash\n",
    "gsutil cp babyweight/dist/babyweight_trainer-0.1.tar.gz gs://${BUCKET}/babyweight/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551dd5bf-95e3-465b-91db-d82ce8ed4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit model training to vertex ai with specific random hyperparameters\n",
    "# and passing them as arguments through .yaml file to task.py file\n",
    "%%bash\n",
    "\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_$TIMESTAMP\n",
    "JOB_NAME=babyweight_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/babyweight/babyweight_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-4\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv\n",
    "    - --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=10000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\n",
    "    - --nembeds=8\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fa17c8c9-57ab-4208-b296-6fe717749bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The training of model in vertex ai code ends here #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58b5cc-6a9e-4da5-b397-c1039053ecf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dded91bb-7ed7-48e8-b234-ef77b303546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following is the code for hyper parameter tuning ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034f0a1-7c95-4e6b-beab-d5a764e013a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfomr hyper parameter tuning using arguments in .YAML file\n",
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "BASE_OUTPUT_DIR=gs://${BUCKET}/babyweight/hp_tuning_$TIMESTAMP\n",
    "JOB_NAME=babyweight_hpt_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/babyweight/babyweight_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./hyperparam.yaml \"displayName: $JOB_NAME\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: val_rmse\n",
    "    goal: MINIMIZE\n",
    "  parameters:\n",
    "  - parameterId: batch_size\n",
    "    integerValueSpec:\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "    scaleType: UNIT_LOG_SCALE\n",
    "  - parameterId: nembeds\n",
    "    integerValueSpec:\n",
    "      minValue: 3\n",
    "      maxValue: 30\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  baseOutputDirectory:\n",
    "    outputUriPrefix: $BASE_OUTPUT_DIR\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: n1-standard-8\n",
    "    pythonPackageSpec:\n",
    "      executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "      packageUris:\n",
    "      - $PYTHON_PACKAGE_URI\n",
    "      pythonModule: $PYTHON_MODULE\n",
    "      args:\n",
    "      - --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv\n",
    "      - --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv\n",
    "      - --num_epochs=10\n",
    "      - --train_examples=5000\n",
    "      - --eval_steps=100\n",
    "      - --batch_size=32\n",
    "      - --nembeds=8\n",
    "    replicaCount: 1\"\n",
    "        \n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --max-trial-count=20 \\\n",
    "    --parallel-trial-count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a923b3-02b0-4c35-87a5-c8af209c389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best hyperparameters after fine-tuning and train the final model\n",
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/babyweight/tuned_$TIMESTAMP\n",
    "JOB_NAME=babyweight_tuned_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/babyweight/babyweight_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./tuned_config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv\n",
    "    - --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=20000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\n",
    "    - --nembeds=8\"\n",
    "    \n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=tuned_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbdeea-3dff-4b5d-bf9a-da2838f6e734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
