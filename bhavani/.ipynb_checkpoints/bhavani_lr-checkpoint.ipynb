{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13740e4-cd6d-4656-a493-087430e7ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21823ca8-3a2b-4f7a-9cb2-4e658da0b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = !gcloud storage ls\n",
    "BUCKET = BUCKET[-1].split(\"//\")[-1]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c92446-59a7-4115-bd62-2bcc2514fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d504809f-029d-4449-998b-6da1f0d9bcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project ${PROJECT}\n",
    "gcloud config set ai/region ${REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb8b4be-0975-4e84-9560-cf8513e6fd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwiklabs-asl-00-c812c3b423f2', 'spectrain/')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT, BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75064d3-fa65-48a6-83f2-1ac51d2abba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p spectrain_csv_dnn/trainer\n",
    "touch spectrain_csv_dnn/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5759047b-5b7d-401f-b95f-95c53376f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spectrain_csv_dnn/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spectrain_csv_dnn/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        default = os.getenv(\"AIP_MODEL_DIR\")\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize_1\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize_2\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 100\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19344ca5-0332-4347-aa0c-89aec3f7ba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8be8f86d-8dad-4b0c-9a44-31a0ed96d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spectrain_csv_dnn/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spectrain_csv_dnn/trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import credentials\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (Conv1D, Dense, Dropout, Flatten, MaxPooling1D, Softmax)\n",
    "\n",
    "# Define the CKD-EPI equation function\n",
    "def calculate_eGFR(row):\n",
    "    if row['Sex'] == 'male':\n",
    "        kappa = 0.9\n",
    "        alpha = -0.302\n",
    "        beta = 1.0\n",
    "    else:\n",
    "        kappa = 0.7\n",
    "        alpha = -0.241\n",
    "        beta = 1.012\n",
    "\n",
    "    eGFR = 142 * min(row['serum_creatinine'] / kappa, 1)**alpha * \\\n",
    "           max(row['serum_creatinine'] / kappa, 1)**(-1.2) * \\\n",
    "           0.9938**row['Patient.Age.at.Biopsy'] * beta\n",
    "    return eGFR\n",
    "\n",
    "def get_add_var(image_input_dir):\n",
    "    df = pd.read_csv(image_input_dir)\n",
    "    df['eGFR'] = df.apply(calculate_eGFR, axis=1)\n",
    "    df['time.TX']=abs(df['Patient.Age.at.Biopsy'] - df['Patient.Age.at.TX'])\n",
    "    eGFR_bins = [float('-inf'), 60, 89, float('inf')]\n",
    "    TimeTX_bins = [float('-inf'), 1, float('inf')]\n",
    "\n",
    "    # Create the binned columns for 'eGFR' and 'Time.TX'\n",
    "    df['eGFR_bin'] = pd.cut(df['eGFR'], bins=eGFR_bins, labels=['<60', '60-89', '>=90'])\n",
    "    df['time.TX_bin'] = pd.cut(df['time.TX'], bins=TimeTX_bins, labels=['<1 year', '>1 year'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"serum_creatinine\",\n",
    "    \"urea\",\n",
    "    \"dimethylamine\",\n",
    "    \"UA.Pro\",\n",
    "    \"phenylacetylglutamine\",\n",
    "    \"Hypertension\",\n",
    "    \"trigonellin\",\n",
    "    \"lactate\",\n",
    "    \"citrate\",\n",
    "    \"hippurate\",\n",
    "    \"Sex\",\n",
    "    \"alanine\",\n",
    "    \"Diabetes\",\n",
    "    \"UA.Hb\",\n",
    "    \"eGFR\",\n",
    "    \"time.TX\",\n",
    "    \"eGFR_bin\",\n",
    "    \"time.TX_bin\",\n",
    "    \"Case\"\n",
    "]\n",
    "LABEL_COLUMN = \"Case\"\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"serum_creatinine\", \"urea\",\"dimethylamine\", \"phenylacetylglutamine\",\n",
    "    \"trigonellin\",\"lactate\",\"citrate\",\"hippurate\",\"alanine\",\"eGFR\",\"time.TX\"]\n",
    "CATEGORICAL_COLUMNS = [\"Sex\", \"Hypertension\", \"eGFR_bin\",\"UA.Pro\", \"UA.Hb\",\"Diabetes\", \"time.TX_bin\"]\n",
    "\n",
    "ONE_HOT_COLS = ['Sex_female', 'Sex_male',\n",
    "       'Hypertension_False', 'Hypertension_True', 'Hypertension_unknown',\n",
    "       'eGFR_bin_<60', 'eGFR_bin_60-89', 'eGFR_bin_>=90', 'UA.Pro_False',\n",
    "       'UA.Pro_True', 'UA.Pro_unknown', 'UA.Hb_False', 'UA.Hb_True',\n",
    "       'UA.Hb_unknown', 'Diabetes_False', 'Diabetes_True', 'Diabetes_unknown',\n",
    "       'time.TX_bin_<1 year', 'time.TX_bin_>1 year']\n",
    "\n",
    "def transform_data(features_df):\n",
    "    \n",
    "    features_df[NUMERICAL_COLUMNS] = features_df[NUMERICAL_COLUMNS].fillna(0)\n",
    "    features_df[CATEGORICAL_COLUMNS] = features_df[CATEGORICAL_COLUMNS].fillna('unknown')\n",
    "    features_df = pd.get_dummies(features_df, columns=CATEGORICAL_COLUMNS, drop_first=True)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features_df[NUMERICAL_COLUMNS])\n",
    "    scaled_features = pd.DataFrame(scaled_features, columns=NUMERICAL_COLUMNS)\n",
    "    \n",
    "    features_df=features_df.drop(columns=NUMERICAL_COLUMNS)\n",
    "    features_df = pd.concat([scaled_features,features_df], axis=1)\n",
    "    \n",
    "    for COLS in ONE_HOT_COLS:\n",
    "        if COLS not in features_df.columns:\n",
    "            features_df[COLS]=0\n",
    "            \n",
    "    return features_df\n",
    "    \n",
    "    \n",
    "\n",
    "def load_dataset(csv_input_dir, batch_size, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    # Make a CSV dataset\n",
    "    df = get_add_var(csv_input_dir)\n",
    "    df=df[CSV_COLUMNS]\n",
    "    features,labels = df,df.pop(LABEL_COLUMN)\n",
    "    features = transform_data(features)\n",
    " \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features.values, labels)).cache().shuffle(len(features)).batch(batch_size)\n",
    "    \n",
    "    # Shuffle and repeat for training\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_model(hidden_units_1, hidden_units_2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hidden_units_1, activation='relu', input_shape=(30,)))\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(hidden_units_2, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "# Instantiate the HyperTune reporting object\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "# Reporting callback\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='auc',\n",
    "            metric_value=logs['val_auc'],\n",
    "            global_step=epoch)\n",
    "        \n",
    "        \n",
    "def train_and_evaluate(args):\n",
    "    model = build_model(args['nnsize_1'], args['nnsize_2'])\n",
    "\n",
    "    trainds = load_dataset(args[\"train_data_path\"], args[\"batch_size\"], tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    evalds = load_dataset(args[\"eval_data_path\"], args[\"batch_size\"], tf.estimator.ModeKeys.EVAL)\n",
    "    \n",
    "    if args[\"eval_steps\"]:\n",
    "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
    "    steps_per_epoch = args[\"train_examples\"] // args[\"batch_size\"]\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/spectrain_csv_dnn\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=args[\"batch_size\"],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,\n",
    "        callbacks=[cp_callback, HPTCallback()])\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b3e60-9e72-4aa8-bef3-2f36a62769c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "685eb4e8-b68c-4c41-80e6-157220387c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 16:32:32.632264: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 16:32:35.331706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-14 16:32:35.331822: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-14 16:32:35.331832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-14 16:32:41.998338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.011073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.012902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.014923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 16:32:42.015992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.017732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.019451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.861885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.863973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.865870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 16:32:42.867575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 16:32:45.289832: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x564a5aa51ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-14 16:32:45.289873: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-06-14 16:32:45.297525: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-14 16:32:45.443407: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 3s - loss: 0.6139 - auc: 0.5665 - val_loss: 0.6525 - val_auc: 0.8542 - 3s/epoch - 287ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.5395 - auc: 0.7115 - val_loss: 0.5160 - val_auc: 0.4688 - 110ms/epoch - 11ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.5979 - auc: 0.6618 - val_loss: 0.4334 - val_auc: 0.9048 - 102ms/epoch - 10ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.5366 - auc: 0.7220 - val_loss: 0.5660 - val_auc: 0.2222 - 88ms/epoch - 9ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.6682 - auc: 0.5643 - val_loss: 0.6750 - val_auc: 0.6250 - 94ms/epoch - 9ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.6223 - auc: 0.5617 - val_loss: 0.7049 - val_auc: 0.3571 - 89ms/epoch - 9ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.6374 - auc: 0.5646 - val_loss: 0.7113 - val_auc: 0.5417 - 88ms/epoch - 9ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.6286 - auc: 0.6036 - val_loss: 0.8500 - val_auc: 0.2143 - 94ms/epoch - 9ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.6658 - auc: 0.5350 - val_loss: 0.5981 - val_auc: 0.8750 - 89ms/epoch - 9ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: saving model to spectrain_csv_dnn_trained/checkpoints/spectrain_csv_dnn\n",
      "10/10 - 0s - loss: 0.5819 - auc: 0.5761 - val_loss: 0.5705 - val_auc: 0.6905 - 89ms/epoch - 9ms/step\n",
      "Exported trained model to spectrain_csv_dnn_trained/20230614163247\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=spectrain_csv_dnn_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/spectrain_csv_dnn\n",
    "python3 -m trainer.task \\\n",
    "    --train_data_path=gs://${BUCKET}bhavani/csv_split/train.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}bhavani/csv_split/valid.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=10 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b183ab-93fa-4693-9c80-a6a6b5a61a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c65fcc-b62b-4a36-9ba8-fc61bcc4be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spectrain_csv_dnn/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='spectrain_csv_dnn_trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='spectrain edge detected image model training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c698804-cb80-4c2a-934b-949087e4e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd spectrain_csv_dnn\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d124352-c4b5-4ea0-898e-0d6c427ba312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071569d-2893-4b95-b695-3655b3de3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp spectrain_csv_dnn/dist/spectrain_csv_dnn_trainer-0.1.tar.gz gs://${BUCKET}/spectrain_csv_dnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c11a4d-60f0-432f-af24-3172d3344f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4810057c-d63a-455f-a216-8468cff9cdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f24a4-28a5-415d-9b2e-b816ffc87834",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/spectrain_csv_dnn/trained_model_$TIMESTAMP\n",
    "JOB_NAME=spectrain_csv_dnn_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/spectrain_csv_dnn/spectrain_csv_dnn_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-4\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/bhavani/csv_split/train.csv\n",
    "    - --eval_data_path=gs://${BUCKET}/bhavani/csv_split/valid.csv\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=10000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dea31-5bad-426c-a8cd-e4b7cf5f0344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2a282-c06f-4d12-ae73-18463f7dc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "BASE_OUTPUT_DIR=gs://${BUCKET}/spectrain_csv_dnn/hp_tuning_$TIMESTAMP\n",
    "JOB_NAME=spectrain_csv_dnn_hpt_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/spectrain_csv_dnn/spectrain_csv_dnn_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./hyperparam.yaml \"displayName: $JOB_NAME\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: val_auc\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: batch_size\n",
    "    integerValueSpec:\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "    scaleType: UNIT_LOG_SCALE\n",
    "  - parameterId: filt_size1\n",
    "    integerValueSpec:\n",
    "      minValue: 16\n",
    "      maxValue: 64\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  - parameterId: filt_size2\n",
    "    integerValueSpec:\n",
    "      minValue: 8\n",
    "      maxValue: 32\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  baseOutputDirectory:\n",
    "    outputUriPrefix: $BASE_OUTPUT_DIR\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: n1-standard-8\n",
    "    pythonPackageSpec:\n",
    "      executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "      packageUris:\n",
    "      - $PYTHON_PACKAGE_URI\n",
    "      pythonModule: $PYTHON_MODULE\n",
    "      args:\n",
    "      - --train_data_path=gs://${BUCKET}/bhavani/csv_split/train.csv\n",
    "      - --eval_data_path=gs://${BUCKET}/bhavani/csv_split/valid.csv\n",
    "      - --num_epochs=10\n",
    "      - --train_examples=5000\n",
    "      - --eval_steps=100\n",
    "      - --batch_size=32\n",
    "    replicaCount: 1\"\n",
    "        \n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --max-trial-count=20 \\\n",
    "    --parallel-trial-count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eddd546-49eb-4870-8d36-c22903399c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54265d2b-fcb5-404a-a3ae-e2cfef3a46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/spectrain_csv_dnn/tuned_$TIMESTAMP\n",
    "JOB_NAME=spectrain_csv_dnn_tuned_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/spectrain_csv_dnn/spectrain_csv_dnn_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./tuned_config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/bhavani/csv_split/train.csv\n",
    "    - --eval_data_path=gs://${BUCKET}/bhavani/csv_split/valid.csv\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=20000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\n",
    "    - --nembeds=8\"\n",
    "    \n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=tuned_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553f1ee-52ef-415c-8fdd-49023ed6d26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
