{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0741bff-7cb4-4f67-a3b2-ab6ff2b998a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8523cc18-96a2-4815-b92b-b8b065b3f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1202a7a9-1b05-4af5-ae12-1dfefc1e587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75798303-2de8-4696-a988-e27a4b94518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blob(blobs):\n",
    "    for blob in blobs:\n",
    "        yield blob\n",
    "        \n",
    "def get_image_paths(image_input_dir):\n",
    "    # initialize the GCS client\n",
    "    image_bucket = image_input_dir.split('/')[2]\n",
    "    prefix_dir = '/'.join(image_input_dir.split('/')[3:])\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # get the storage bucket\n",
    "    bucket = storage_client.get_bucket(image_bucket)\n",
    "\n",
    "    image_paths=[]\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(image_bucket, prefix=prefix_dir)\n",
    "    \n",
    "    for blob in get_blob(blobs):\n",
    "        if \"output\" in blob.name:\n",
    "            \n",
    "            image_paths.append('gs://spectrain/'+blob.name)\n",
    "            \n",
    "    return image_paths\n",
    "\n",
    "def load_images(imagePath):\n",
    "    # read the image from disk, decode it, convert the data type to\n",
    "    # floating point, and resize it\n",
    "    \n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, (256,256))\n",
    "    # parse the class label from the file path\n",
    "    label = tf.strings.split(imagePath, os.path.sep)[-2]\n",
    "    if label=='positive':\n",
    "        label=1\n",
    "    else:\n",
    "        label=0\n",
    "\n",
    "    # return the image and the label\n",
    "    return (image, label)\n",
    "\n",
    "def load_dataset(images_dir, batch_size, training):\n",
    "    \n",
    "    filePaths = get_image_paths(image_input_dir=images_dir)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(filePaths)\n",
    "    ds = (ds\n",
    "        .shuffle(len(filePaths))\n",
    "        .map(load_images)\n",
    "        .cache()\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        return ds.repeat()\n",
    "    else:\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86fa3eb1-7a40-479b-a8a7-53bf4cbb7345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 06:14:35.388013: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 06:14:38.209818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-14 06:14:38.209915: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-14 06:14:38.209925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "import numpy as np\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import credentials\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (Conv1D, Dense, Dropout, Flatten, MaxPooling1D, Softmax)\n",
    "def build_model(filter_size_1, filter_size_2, kernel_size, pool_kernel_size, hidden_units_1, hidden_units_2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu', input_shape=(256, 256)))\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size))\n",
    "    model.add(Conv1D(filter_size_2, kernel_size=kernel_size,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50cf8b50-235d-4fa2-9c2f-aea3660f54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 06:14:59.409138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:14:59.421237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:14:59.422862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:14:59.425117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 06:14:59.426158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:14:59.427834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:14:59.429474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:15:00.282170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:15:00.284106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:15:00.285715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 06:15:00.287279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = build_model(filter_size_1=16, filter_size_2=8, \n",
    "                        kernel_size=2, pool_kernel_size=2\n",
    "                        , hidden_units_1=128, hidden_units_2=128)\n",
    "\n",
    "trainds = load_dataset(images_dir=\"gs://spectrain/bhavani/valid_images\", batch_size=16, training=True)\n",
    "evalds = load_dataset(images_dir=\"gs://spectrain/bhavani/test_images\", batch_size=16, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "527b8f6e-d1ab-4b9a-9cc0-1261fbec3c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "10/10 - 0s - loss: 0.2117 - auc: 0.9988 - val_loss: 1.1370 - val_auc: 0.5530 - 111ms/epoch - 11ms/step\n",
      "Epoch 2/16\n",
      "10/10 - 0s - loss: 0.0865 - auc: 1.0000 - val_loss: 1.3106 - val_auc: 0.5522 - 104ms/epoch - 10ms/step\n",
      "Epoch 3/16\n",
      "10/10 - 0s - loss: 0.0223 - auc: 1.0000 - val_loss: 2.0109 - val_auc: 0.5697 - 107ms/epoch - 11ms/step\n",
      "Epoch 4/16\n",
      "10/10 - 0s - loss: 0.0112 - auc: 1.0000 - val_loss: 3.3081 - val_auc: 0.5724 - 106ms/epoch - 11ms/step\n",
      "Epoch 5/16\n",
      "10/10 - 0s - loss: 0.0321 - auc: 0.9991 - val_loss: 3.4563 - val_auc: 0.5413 - 103ms/epoch - 10ms/step\n",
      "Epoch 6/16\n",
      "10/10 - 0s - loss: 0.0279 - auc: 0.9996 - val_loss: 2.0279 - val_auc: 0.5425 - 105ms/epoch - 10ms/step\n",
      "Epoch 7/16\n",
      "10/10 - 0s - loss: 0.0708 - auc: 0.9991 - val_loss: 2.3020 - val_auc: 0.5319 - 104ms/epoch - 10ms/step\n",
      "Epoch 8/16\n",
      "10/10 - 0s - loss: 0.1123 - auc: 0.9909 - val_loss: 3.1677 - val_auc: 0.5557 - 105ms/epoch - 11ms/step\n",
      "Epoch 9/16\n",
      "10/10 - 0s - loss: 0.0782 - auc: 0.9914 - val_loss: 1.9534 - val_auc: 0.5179 - 106ms/epoch - 11ms/step\n",
      "Epoch 10/16\n",
      "10/10 - 0s - loss: 0.0184 - auc: 1.0000 - val_loss: 2.7281 - val_auc: 0.5360 - 103ms/epoch - 10ms/step\n",
      "Epoch 11/16\n",
      "10/10 - 0s - loss: 0.0117 - auc: 1.0000 - val_loss: 2.1959 - val_auc: 0.5190 - 103ms/epoch - 10ms/step\n",
      "Epoch 12/16\n",
      "10/10 - 0s - loss: 0.0042 - auc: 1.0000 - val_loss: 2.2108 - val_auc: 0.5255 - 104ms/epoch - 10ms/step\n",
      "Epoch 13/16\n",
      "10/10 - 0s - loss: 0.0027 - auc: 1.0000 - val_loss: 2.3593 - val_auc: 0.5250 - 101ms/epoch - 10ms/step\n",
      "Epoch 14/16\n",
      "10/10 - 0s - loss: 0.0011 - auc: 1.0000 - val_loss: 2.5032 - val_auc: 0.5158 - 102ms/epoch - 10ms/step\n",
      "Epoch 15/16\n",
      "10/10 - 0s - loss: 7.3376e-04 - auc: 1.0000 - val_loss: 2.5905 - val_auc: 0.5159 - 104ms/epoch - 10ms/step\n",
      "Epoch 16/16\n",
      "10/10 - 0s - loss: 6.3762e-04 - auc: 1.0000 - val_loss: 2.6489 - val_auc: 0.5205 - 104ms/epoch - 10ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "# Reporting callback\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='auc',\n",
    "            metric_value=logs['val_auc'],\n",
    "            global_step=epoch)\n",
    "        \n",
    "history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=16,\n",
    "        steps_per_epoch=10,\n",
    "        verbose=2,\n",
    "        validation_steps=10,\n",
    "    callbacks=[HPTCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32355a46-6563-488e-a249-492b083394ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23023f06-cda0-4d27-880f-63ba8694200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = !gcloud config list --format 'value(core.project)'\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = !gcloud storage ls\n",
    "BUCKET = BUCKET[-1].split(\"//\")[-1]\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e13f1bed-df70-4f1e-84d6-644fd8d019bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f5bc45d-3645-41fd-b02d-d008791d78ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project ${PROJECT}\n",
    "gcloud config set ai/region ${REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f197897c-5c83-4fd5-9a47-88f1df9f29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an init file to identify the following code as a package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69e19587-fb56-43ee-9c72-79c87fa2bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p spectrain_proc_img/trainer\n",
    "touch spectrain_proc_img/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e751686a-58f2-40be-ad0f-e1a9f352cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file to parse the arguments\n",
    "# We will use this later to parse arguments when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf0269df-181b-40c1-9b94-1b5fffbc10e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spectrain_proc_img/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spectrain_proc_img/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        default = os.getenv(\"AIP_MODEL_DIR\")\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize_1\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize_2\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ksize\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=4\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pool_ksize\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--filt_size1\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--filt_size2\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 100\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a78ba8b-004d-4a1e-9a9b-c6fb12f1a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place all the preprocessing, model building, training and evaluation code in this cell to package in\n",
    "# model.py to later train directly in vertex ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90bf785d-bc71-4b57-ba26-aee9c465d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spectrain_proc_img/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spectrain_proc_img/trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "import numpy as np\n",
    "from google.cloud import bigquery, storage\n",
    "from google.oauth2 import credentials\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (Conv1D, Dense, Dropout, Flatten, MaxPooling1D, Softmax)\n",
    "\n",
    "def get_blob(blobs):\n",
    "    for blob in blobs:\n",
    "        yield blob\n",
    "        \n",
    "def get_image_paths(image_input_dir):\n",
    "    # initialize the GCS client\n",
    "    image_bucket = image_input_dir.split('/')[2]\n",
    "    prefix_dir = '/'.join(image_input_dir.split('/')[3:])\n",
    "    prefix_dir = prefix_dir[1:]\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # get the storage bucket\n",
    "    bucket = storage_client.get_bucket(image_bucket)\n",
    "\n",
    "    image_paths=[]\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(image_bucket, prefix=prefix_dir)\n",
    "    \n",
    "    for blob in get_blob(blobs):\n",
    "        if \"output\" in blob.name:\n",
    "            image_paths.append('gs://spectrain/'+blob.name)\n",
    "            \n",
    "    return image_paths\n",
    "\n",
    "def load_images(imagePath):\n",
    "    # read the image from disk, decode it, convert the data type to\n",
    "    # floating point, and resize it\n",
    "    \n",
    "    image = tf.io.read_file(imagePath)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, (256,256))\n",
    "    # parse the class label from the file path\n",
    "    label = tf.strings.split(imagePath, os.path.sep)[-2]\n",
    "    if label=='positive':\n",
    "        label=1\n",
    "    else:\n",
    "        label=0\n",
    "    # return the image and the label\n",
    "    return (image, label)\n",
    "\n",
    "def load_dataset(images_dir, batch_size, training):\n",
    "    \n",
    "    filePaths = get_image_paths(image_input_dir=images_dir)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(filePaths)\n",
    "    ds = (ds\n",
    "        .map(load_images)\n",
    "        .cache()\n",
    "        .shuffle(len(filePaths))\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    if training:\n",
    "        return ds.repeat()\n",
    "    else:\n",
    "        return ds\n",
    "\n",
    "def build_model(filter_size_1, filter_size_2, kernel_size, pool_kernel_size, hidden_units_1, hidden_units_2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu', input_shape=(256, 256)))\n",
    "    model.add(Conv1D(filter_size_1, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size))\n",
    "    model.add(Conv1D(filter_size_2, kernel_size=kernel_size,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_kernel_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_1, activation='relu'))\n",
    "    model.add(Dense(hidden_units_2, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['AUC'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "# Instantiate the HyperTune reporting object\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "# Reporting callback\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='auc',\n",
    "            metric_value=logs['val_auc'],\n",
    "            global_step=epoch)\n",
    "        \n",
    "        \n",
    "def train_and_evaluate(args):\n",
    "    model = build_model(filter_size_1=args[\"filt_size1\"], filter_size_2=args[\"filt_size2\"], \n",
    "                        kernel_size=args['ksize'], pool_kernel_size=args['pool_ksize']\n",
    "                        , hidden_units_1=args['nnsize_1'], hidden_units_2=args['nnsize_2'])\n",
    "\n",
    "    trainds = load_dataset(args[\"train_data_path\"], args[\"batch_size\"], training=True)\n",
    "\n",
    "    evalds = load_dataset(args[\"eval_data_path\"], args[\"batch_size\"], training=False)\n",
    "    \n",
    "    if args[\"eval_steps\"]:\n",
    "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
    "    steps_per_epoch = args[\"train_examples\"] // args[\"batch_size\"]\n",
    "    \n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/spectrain_proc_img\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=args[\"batch_size\"],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,\n",
    "        callbacks=[cp_callback, HPTCallback()])\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "515dfc22-80a0-4f93-a64f-d61f27884d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train the model locally to check if everything is good #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05a73fdf-1616-4847-bf24-9b2fcf345302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 07:05:07.967708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 07:05:10.740396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-14 07:05:10.740534: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-14 07:05:10.740557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-14 07:05:14.894401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:14.907742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:14.909582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:14.911679: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 07:05:14.913047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:14.914775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:14.916488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:15.815898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:15.818054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:15.819980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-14 07:05:15.821834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 181 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2023-06-14 07:05:15.843051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 181.50M (190316544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 07:05:27.683796: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 105 of 1064\n",
      "2023-06-14 07:05:37.679213: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 210 of 1064\n",
      "2023-06-14 07:05:47.654155: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 313 of 1064\n",
      "2023-06-14 07:05:57.656797: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 423 of 1064\n",
      "2023-06-14 07:06:07.699094: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 521 of 1064\n",
      "2023-06-14 07:06:17.866936: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 629 of 1064\n",
      "2023-06-14 07:06:27.642595: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 735 of 1064\n",
      "2023-06-14 07:06:37.669440: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 844 of 1064\n",
      "2023-06-14 07:06:47.688406: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 950 of 1064\n",
      "2023-06-14 07:06:57.756272: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 1061 of 1064\n",
      "2023-06-14 07:06:57.967817: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2023-06-14 07:06:58.043440: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-06-14 07:06:58.043529: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1152 : UNIMPLEMENTED: DNN library is not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/asl/bhavani/spectrain_proc_img/trainer/task.py\", line 94, in <module>\n",
      "    model.train_and_evaluate(arguments)\n",
      "  File \"/home/jupyter/asl/bhavani/spectrain_proc_img/trainer/model.py\", line 125, in train_and_evaluate\n",
      "    history = model.fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 52, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\n",
      "\n",
      "Detected at node 'sequential/conv1d/Conv1D' defined at (most recent call last):\n",
      "    File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/home/jupyter/asl/bhavani/spectrain_proc_img/trainer/task.py\", line 94, in <module>\n",
      "      model.train_and_evaluate(arguments)\n",
      "    File \"/home/jupyter/asl/bhavani/spectrain_proc_img/trainer/model.py\", line 125, in train_and_evaluate\n",
      "      history = model.fit(\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n",
      "      tmp_logs = self.train_function(iterator)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n",
      "      outputs = model.train_step(data)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n",
      "      y_pred = self(x, training=True)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n",
      "      return super().__call__(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/sequential.py\", line 413, in call\n",
      "      return super().call(inputs, training=training, mask=mask)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n",
      "      return self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n",
      "      outputs = node.layer(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n",
      "      outputs = call_fn(inputs, *args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n",
      "      outputs = self.convolution_op(inputs, self.kernel)\n",
      "    File \"/opt/conda/lib/python3.10/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n",
      "      return tf.nn.convolution(\n",
      "Node: 'sequential/conv1d/Conv1D'\n",
      "DNN library is not found.\n",
      "\t [[{{node sequential/conv1d/Conv1D}}]] [Op:__inference_train_function_2265]\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'OUTDIR=spectrain_proc_img_trained\\nrm -rf ${OUTDIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/spectrain_proc_img\\npython3 -m trainer.task \\\\\\n    --train_data_path=gs://${BUCKET}/bhavani/train_images \\\\\\n    --eval_data_path=gs://${BUCKET}/bhavani/valid_images \\\\\\n    --output_dir=${OUTDIR} \\\\\\n    --batch_size=10 \\\\\\n    --num_epochs=1 \\\\\\n    --train_examples=10 \\\\\\n    --eval_steps=10\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOUTDIR=spectrain_proc_img_trained\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mrm -rf $\u001b[39;49m\u001b[38;5;132;43;01m{OUTDIR}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexport PYTHONPATH=$\u001b[39;49m\u001b[38;5;132;43;01m{PYTHONPATH}\u001b[39;49;00m\u001b[38;5;124;43m:$\u001b[39;49m\u001b[38;5;132;43;01m{PWD}\u001b[39;49;00m\u001b[38;5;124;43m/spectrain_proc_img\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython3 -m trainer.task \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --train_data_path=gs://$\u001b[39;49m\u001b[38;5;132;43;01m{BUCKET}\u001b[39;49;00m\u001b[38;5;124;43m/bhavani/train_images \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval_data_path=gs://$\u001b[39;49m\u001b[38;5;132;43;01m{BUCKET}\u001b[39;49;00m\u001b[38;5;124;43m/bhavani/valid_images \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --output_dir=$\u001b[39;49m\u001b[38;5;132;43;01m{OUTDIR}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --batch_size=10 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --num_epochs=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --train_examples=10 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --eval_steps=10\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2477\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2478\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'OUTDIR=spectrain_proc_img_trained\\nrm -rf ${OUTDIR}\\nexport PYTHONPATH=${PYTHONPATH}:${PWD}/spectrain_proc_img\\npython3 -m trainer.task \\\\\\n    --train_data_path=gs://${BUCKET}/bhavani/train_images \\\\\\n    --eval_data_path=gs://${BUCKET}/bhavani/valid_images \\\\\\n    --output_dir=${OUTDIR} \\\\\\n    --batch_size=10 \\\\\\n    --num_epochs=1 \\\\\\n    --train_examples=10 \\\\\\n    --eval_steps=10\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=spectrain_proc_img_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/spectrain_proc_img\n",
    "python3 -m trainer.task \\\n",
    "    --train_data_path=gs://${BUCKET}/bhavani/train_images \\\n",
    "    --eval_data_path=gs://${BUCKET}/bhavani/valid_images \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=10 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=10 \\\n",
    "    --eval_steps=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c20413-f546-4a0b-a2e0-7a5a8d1dc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The following is the code to train the model on vertex ai with a randomly selected hyperparameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3f7b1-40b7-4d4a-bfc4-5b99b3235ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a package source distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bded03e6-407b-4622-87d9-fa20a57db433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spectrain_proc_img/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spectrain_proc_img/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='spectrain_proc_img_trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='spectrain edge detected image model training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344372e-f6d2-4707-b7a4-42c04e17e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local directory to store source distribution package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adeff8-b4ba-4351-b2ef-ce0e1959deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd spectrain_proc_img\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259309e-505c-483c-ba01-750922c6a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the package in the cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95440c-3101-4f45-8fe4-b3f971b870e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp spectrain_proc_img/dist/spectrain_proc_img_trainer-0.1.tar.gz gs://${BUCKET}/spectrain_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f76f77-b8d7-4688-bcaf-b2208f9a22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit model training to vertex ai with specific random hyperparameters\n",
    "# and passing them as arguments through .yaml file to task.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58666c-86a9-4dfa-9806-35e7b09c52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/spectrain_cnn/trained_model_$TIMESTAMP\n",
    "JOB_NAME=spectrain_cnn_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/spectrain_cnn/spectrain_proc_img_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-4\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/bhavani/train_images\n",
    "    - --eval_data_path=gs://${BUCKET}/bhavani/valid_images\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=10000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\"\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3e5fa-871d-476c-9bf3-9a84e9638bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The training of model in vertex ai code ends here #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4cdb6-e9fa-4d63-b3ba-c4d9e81bd2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3847d49-4fc7-417a-88e3-ac5adb181312",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following is the code for hyper parameter tuning ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544333d7-41aa-4a6e-bfad-68bad5b0abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfomr hyper parameter tuning using arguments in .YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fea57-902d-4299-8473-a1a42aa5e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "BASE_OUTPUT_DIR=gs://${BUCKET}/spectrain_cnn/hp_tuning_$TIMESTAMP\n",
    "JOB_NAME=spectrain_cnn_hpt_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/spectrain_cnn/spectrain_proc_img_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./hyperparam.yaml \"displayName: $JOB_NAME\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: val_rmse\n",
    "    goal: MINIMIZE\n",
    "  parameters:\n",
    "  - parameterId: batch_size\n",
    "    integerValueSpec:\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "    scaleType: UNIT_LOG_SCALE\n",
    "  - parameterId: nembeds\n",
    "    integerValueSpec:\n",
    "      minValue: 3\n",
    "      maxValue: 30\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  baseOutputDirectory:\n",
    "    outputUriPrefix: $BASE_OUTPUT_DIR\n",
    "  workerPoolSpecs:\n",
    "  - machineSpec:\n",
    "      machineType: n1-standard-8\n",
    "    pythonPackageSpec:\n",
    "      executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "      packageUris:\n",
    "      - $PYTHON_PACKAGE_URI\n",
    "      pythonModule: $PYTHON_MODULE\n",
    "      args:\n",
    "      - --train_data_path=gs://${BUCKET}/bhavani/train_images\n",
    "      - --eval_data_path=gs://${BUCKET}/bhavani/valid_images\n",
    "      - --num_epochs=10\n",
    "      - --train_examples=5000\n",
    "      - --eval_steps=100\n",
    "      - --batch_size=32\n",
    "      - --nembeds=8\n",
    "    replicaCount: 1\"\n",
    "        \n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --max-trial-count=20 \\\n",
    "    --parallel-trial-count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef13033-bea7-4996-b959-25262beae788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41883f4-b81b-42ed-8527-4e0ff38b498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best hyperparameters after fine-tuning and train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c063c18-ce1f-48bb-aceb-14e291dd4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\n",
    "OUTDIR=gs://${BUCKET}/spectrain_cnn/tuned_$TIMESTAMP\n",
    "JOB_NAME=spectrain_cnn_tuned_$TIMESTAMP\n",
    "\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/spectrain_cnn/spectrain_proc_img_trainer-0.1.tar.gz\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    "PYTHON_MODULE=trainer.task\n",
    "\n",
    "echo > ./tuned_config.yaml \"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/bhavani/train_images\n",
    "    - --eval_data_path=gs://${BUCKET}/bhavani/valid_images\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=10\n",
    "    - --train_examples=20000\n",
    "    - --eval_steps=100\n",
    "    - --batch_size=32\n",
    "    - --nembeds=8\"\n",
    "    \n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=tuned_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393028c-c033-49cf-8d5b-e18fb250aa12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
